{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38087006",
   "metadata": {},
   "source": [
    "üî• First Concept: NLP - Tokenization\n",
    "\n",
    "üí° What is it?\n",
    "Tokenization = Breaking down text into smaller pieces (words, subwords, or characters) so that models can understand and process them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cc9e1",
   "metadata": {},
   "source": [
    "üß† Why it matters?\n",
    "Machines can‚Äôt read sentences directly. They need tokens (like ‚ÄúHello‚Äù ‚Üí [Hello]) to convert into numbers (embeddings).\n",
    "\n",
    "üîß Types of Tokenizers:\n",
    "\n",
    "- Word-level ‚Üí ‚ÄúHello world‚Äù ‚Üí ['Hello', 'world']\n",
    "- Subword-level (BPE) ‚Üí ‚Äúunhappiness‚Äù ‚Üí ['un', 'happi', 'ness']\n",
    "- Character-level ‚Üí ‚ÄúHi‚Äù ‚Üí ['H', 'i']\n",
    "\n",
    "‚úÖ Real Use:\n",
    "Hugging Face models like BERT use subword tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e6e0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "ü§ó Transformers ‚Äì Core Concept\n",
    "\n",
    "üí° What is a Transformer?\n",
    "A Transformer is an architecture that understands sequences (like sentences) using self-attention ‚Äì it looks at all words at once and learns which ones matter most.\n",
    "\n",
    "üß† Why It Matters?\n",
    "This powers BERT, GPT, Claude, Gemini ‚Äì all modern LLMs.\n",
    "\n",
    "Key Ideas:\n",
    "\n",
    "- No loops, just attention\n",
    "- Parallel processing = Fast\n",
    "- Can understand long-range word relationships (e.g., ‚Äúbank‚Äù = riverbank or money)\n",
    "\n",
    "üß± Transformer Parts (simple view):\n",
    "- Input Embeddings: Text ‚Üí Vectors\n",
    "- Positional Encoding: Adds word order info\n",
    "- Self-Attention: Learns context\n",
    "- Feed Forward Layers: Processes info\n",
    "- Output: Classifies, generates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb56a5b",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "\n",
    "# change in notepad\n",
    "# or change in system Move Python 3.13 to Top in environmental variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c51976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/usr/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshathushetty/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/rakshathushetty/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4033f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998753070831299}]\n"
     ]
    }
   ],
   "source": [
    "# ü§ó Using Hugging Face Transformers (Hands-On)\n",
    "# Let‚Äôs load a real model and run it on your own text üëá\n",
    "\n",
    "from transformers import pipeline    # Correct\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# result = classifier(\"I love lesarning huggface with chatgpt\")\n",
    "result = classifier(\"happy\")\n",
    "print(result) # [{'label': 'POSITIVE', 'score': 0.9979}] 'label': Predicted class 'score': Confidence (close to 1.0 = very confident)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cef83",
   "metadata": {},
   "source": [
    "üß† What‚Äôs happening here:\n",
    "pipeline(\"sentiment-analysis\"): Loads a pretrained model like BERT that‚Äôs fine-tuned for sentiment.\n",
    "\n",
    "You pass in raw text ‚Üí it gets tokenized, embedded, processed by transformer ‚Üí gives a label (POSITIVE/NEGATIVE) + confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d91548",
   "metadata": {},
   "source": [
    "üß† Token IDs & Attention Mask (Mini Concept)\n",
    "\n",
    "üí° Token IDs:\n",
    "- Text is turned into numbers. Example:\n",
    "- \"AI is great\" ‚Üí [101, 9932, 2003, 2307, 102] (Each word/subword gets a unique ID from the model's vocab)\n",
    "\n",
    "üí° Attention Mask:\n",
    "- Tells the model which tokens to focus on (1 = real word, 0 = padding).\n",
    "- Useful when inputs are of different lengths but sent as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc2184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  9932,  2003, 12476,   999,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer #loads class to fetch tokenizer from Hugging Face (e.g. bert, gpt, etc.)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #‚úÖ Downloads the BERT tokenizer (lowercase version) that knows how to:\n",
    "# split words into subwords\n",
    "# convert to token IDs\n",
    "inputs = tokenizer(\"AI is awesome!\", padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# ‚úÖ Tokenizes the input:\n",
    "# padding=True ‚Üí Pads the input if it's shorter than max length\n",
    "# truncation=True ‚Üí Cuts if it‚Äôs too long\n",
    "# return_tensors=\"pt\" ‚Üí Returns PyTorch tensor format (pt = PyTorch)\n",
    "\n",
    "print(inputs['input_ids'])       # Token IDs\n",
    "print(inputs['attention_mask'])  # 1s = real tokens, 0s = ignore (pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e2175",
   "metadata": {},
   "source": [
    "we‚Äôll manually run text through a Transformer model for classification ‚Äî to see how all pieces (tokenizer + model) work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3246,  4.6837]]), hidden_states=None, attentions=None)\n",
      "tensor([[1.2238e-04, 9.9988e-01]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification # Loads tokenizer + classification model\n",
    "import torch # We'll use PyTorch tensors for input/output\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # Tokenizer: Breaks input text into token IDs\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\") #Model: A DistilBERT already trained for sentiment analysis (SST-2 dataset)\n",
    "\n",
    "inputs = tokenizer(\"I love this movie!\", return_tensors=\"pt\") #Tokenizes text ‚Üí returns token IDs + attention mask (in PyTorch tensor format)\n",
    "\n",
    "with torch.no_grad():             # Disables gradient tracking (we're just predicting)\n",
    "    outputs = model(**inputs)     # Passes the input into the model to get output logits\n",
    "print(outputs)\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) #Converts raw output (logits) into probabilities\n",
    "print(predictions)  #Usually returns 2 classes: [negative_prob, positive_prob]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c99a33",
   "metadata": {},
   "source": [
    "### üîπ What is `logits`?\n",
    "\n",
    "* `logits` are the **raw, unnormalized outputs** from the final layer of a neural network.\n",
    "* They can be **positive or negative**, and **don‚Äôt sum to 1**.\n",
    "* You convert `logits` ‚Üí probabilities using `softmax`.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "logits = tensor([[2.0, 0.5]])\n",
    "# After softmax ‚Üí [0.82, 0.18] ‚Üí means class 0 is 82% likely\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What is `**inputs` inside the model?\n",
    "\n",
    "When you do:\n",
    "\n",
    "```python\n",
    "outputs = model(**inputs)\n",
    "```\n",
    "\n",
    "It‚Äôs the same as writing:\n",
    "\n",
    "```python\n",
    "outputs = model(input_ids=..., attention_mask=...)\n",
    "```\n",
    "\n",
    "‚úîÔ∏è `tokenizer(...)` returns a dictionary like:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'input_ids': tensor([[101, 1045, 2293, 2023, 3185, 999, 102]]),\n",
    "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])\n",
    "}\n",
    "```\n",
    "\n",
    "The `**inputs` syntax **unpacks** that dictionary directly into keyword arguments for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9850fa",
   "metadata": {},
   "source": [
    "\n",
    "### 1Ô∏è‚É£ **Why `torch.no_grad()`?**\n",
    "\n",
    "When you're **only predicting (inference)** and not training, you don‚Äôt need to calculate gradients.\n",
    "\n",
    "‚úÖ **Benefits:**\n",
    "\n",
    "* Saves memory\n",
    "* Speeds up execution\n",
    "* Cleaner and safer for inference\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **What is Softmax?**\n",
    "\n",
    "üß† **Softmax** turns raw scores (logits) into probabilities that add up to 1.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "It gives:\n",
    "\n",
    "* High confidence for the most likely class\n",
    "* Low values for others\n",
    "* Output like: `[0.02, 0.98]` ‚Üí 98% confidence for class 1\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Whole Purpose Recap**\n",
    "\n",
    "We‚Äôre doing this:\n",
    "\n",
    "**Raw Text** ‚Üí `Tokenizer` ‚Üí `Model` ‚Üí `Logits` ‚Üí `Softmax` ‚Üí `Probabilities` ‚Üí `Prediction`\n",
    "\n",
    "üìå This is how Hugging Face models work internally:\n",
    "\n",
    "* Tokenization = preprocess\n",
    "* Model = neural network\n",
    "* Logits = raw model output\n",
    "* Softmax = make predictions human-readable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d8063",
   "metadata": {},
   "source": [
    "üõ†Ô∏è Mini Project: Sentiment Classifier for Multiple Texts\n",
    "create a custom function that can analyze multiple reviews at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998781681060791}, {'label': 'NEGATIVE', 'score': 0.9997876286506653}, {'label': 'NEGATIVE', 'score': 0.9886063933372498}, {'label': 'NEGATIVE', 'score': 0.998314619064331}, {'label': 'POSITIVE', 'score': 0.999871015548706}]\n",
      "\n",
      "This movie was fantastic! -> POSITIVE (99.99%)\n",
      "Worst experience ever. -> NEGATIVE (99.98%)\n",
      "I loved the visuals but hated the story. -> NEGATIVE (98.86%)\n",
      "Just average, nothing special. -> NEGATIVE (99.83%)\n",
      "Absolutely brilliant! -> POSITIVE (99.99%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis model\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Sample reviews\n",
    "reviews = [\n",
    "    \"This movie was fantastic!\",\n",
    "    \"Worst experience ever.\",\n",
    "    \"I loved the visuals but hated the story.\",\n",
    "    \"Just average, nothing special.\",\n",
    "    \"Absolutely brilliant!\"\n",
    "]\n",
    "\n",
    "# Analyze all\n",
    "results = sentiment_model(reviews)\n",
    "print(results, end=\"\\n\\n\")\n",
    "\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"{review} -> {result['label']} ({round(result['score']*100, 2)}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335b7e5",
   "metadata": {},
   "source": [
    "üîç What You Practiced:\n",
    "\n",
    "- Multi-input processing\n",
    "- Model confidence score\n",
    "- Basic NLP automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9481c9",
   "metadata": {},
   "source": [
    "üß† Use Custom Models from Hugging Face Hub\n",
    "explore other powerful models (e.g. emotion, topic, toxicity detection) using a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ac4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshathushetty/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/rakshathushetty/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'joy', 'score': 0.7351986765861511}]]\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example 1: Emotion Detection\n",
    "from transformers import pipeline\n",
    "\n",
    "emotion = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=1)\n",
    "\n",
    "print(emotion(\"I am so proud of myself today!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cfe6fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Apple is releasing a new iPhone this year', 'labels': ['technology', 'food'], 'scores': [0.9958313703536987, 0.004168595653027296]}\n"
     ]
    }
   ],
   "source": [
    "# # ‚úÖ Example 2: Topic Classification\n",
    "# topic = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# text = \"Apple is releasing a new iPhone this year\"\n",
    "# # labels = [\"sports\", \"politics\", \"technology\", \"food\"]\n",
    "# labels = [\"technology\", \"food\"]\n",
    "\n",
    "# print(topic(text, candidate_labels=labels))\n",
    "\n",
    "# ‚úÖ Example 2: Topic Classification\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-1\") #light weight model - model=\"typeform/distilbert-base-uncased-mnli\"\n",
    "\n",
    "text = \"Apple is releasing a new iPhone this year\"\n",
    "# labels = [\"sports\", \"politics\", \"technology\", \"food\"]\n",
    "labels = [\"technology\", \"food\"]\n",
    "\n",
    "result = classifier(text, candidate_labels=labels)\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b84ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'toxic', 'score': 0.95553058385849}]\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Example 3: Toxicity Detection\n",
    "toxic = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
    "\n",
    "print(toxic(\"I hate you!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c719bd4",
   "metadata": {},
   "source": [
    "üîç Summary:\n",
    "- pipeline() makes any model easy to use\n",
    "- You can swap models using Hugging Face model names\n",
    "- Explore huggingface.co/models for more tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db165370",
   "metadata": {},
   "source": [
    "zero-shot takes time\n",
    "‚è±Ô∏è Why Zero-Shot Topic Classification Is Slow:\n",
    "You're using this:\n",
    "\n",
    "# pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "This:\n",
    "- Loads a large transformer (BART = 400M+ parameters)\n",
    "- Internally runs the input once per label (e.g., 4 labels = 4 forward passes)\n",
    "- Works without any prior training on your custom labels = ‚Äúzero-shot‚Äù\n",
    "\n",
    "‚úÖ Tips to Speed It Up:\n",
    "\n",
    "- Use fewer candidate labels\n",
    "- Use DistilBART (if available) instead of full BART\n",
    "- Run it only once and cache results locally\n",
    "- Use GPU if you have one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ea483",
   "metadata": {},
   "source": [
    "This warning and retry log is happening because the zero-shot model is trying to download large model files from Hugging Face, but:\n",
    "\n",
    "‚ùóIssue:\n",
    "DNS lookup is failing due to network problems, causing:\n",
    "\n",
    "‚ö†Ô∏è Retrying multiple times\n",
    "‚ö†Ô∏è Delayed or stuck execution\n",
    "üì¶ Big files not loading (models are 1GB+)\n",
    "\n",
    "üîß How to Fix It:\n",
    "‚úÖ Option 1: Set this to silence tokenizer warnings\n",
    "Add this before importing pipelines:\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "‚úÖ Option 2: Try lightweight model\n",
    "Use \"valhalla/distilbart-mnli-12-1\" instead of \"facebook/bart-large-mnli\":\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-1\")\n",
    "\n",
    "‚úÖ Option 3: Restart kernel + run with stable internet\n",
    "Sometimes Colab/VS Code gets stuck due to a corrupted download. Restarting + running again helps.\n",
    "\n",
    "Network/DNS Issues - ping huggingface.co\n",
    "Ensure your environment can connect to Hugging Face:\n",
    "\n",
    "Cache & Retry\n",
    "Clear the cache folder: - ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c845757",
   "metadata": {},
   "source": [
    "Embeddings + Vector Search (FAISS, Pinecone) ‚Äî the foundation of RAG, semantic search, chatbot memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734719e8",
   "metadata": {},
   "source": [
    "**Embeddings + Vector Search**, the backbone of:\n",
    "\n",
    "* **Semantic Search**\n",
    "* **Chatbot memory**\n",
    "* **RAG (Retrieval-Augmented Generation)**\n",
    "\n",
    "Let‚Äôs start with the core idea üëá\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Are Embeddings?\n",
    "\n",
    "Embeddings are **vector representations of text** (or images, code, etc.) in a high-dimensional space.\n",
    "Words/paragraphs with **similar meaning ‚Üí closer vectors**.\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "* \"king\" and \"queen\" will have similar vectors.\n",
    "* \"dog\" and \"bark\" will be closer than \"dog\" and \"car\".\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Why Use Embeddings?\n",
    "\n",
    "* To **compare semantic meaning** (not exact words)\n",
    "* To build **semantic search engines**, chatbot memory, document similarity tools, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è What is FAISS?\n",
    "\n",
    "FAISS (by Facebook AI) lets you:\n",
    "\n",
    "* Store millions of embeddings\n",
    "* Search fast: ‚ÄúWhich vector is most similar to this one?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Flow (RAG / Semantic Search):\n",
    "\n",
    "1. Convert text ‚Üí embeddings (using models like `sentence-transformers`)\n",
    "2. Store them in FAISS or Pinecone\n",
    "3. When user queries:\n",
    "   ‚Üí Convert query to embedding\n",
    "   ‚Üí Search nearest documents\n",
    "   ‚Üí Show or feed them to LLM\n",
    "\n",
    "---\n",
    "Step 1: pip install sentence-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bca2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 2: Get sentence embeddings\n",
    "# Here‚Äôs how to convert text into a vector using a pretrained model: text->vector\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Light & fast, 384-dim embeddings\n",
    "\n",
    "sentences = [\n",
    "    \"I love playing football.\",\n",
    "    \"Soccer is my favorite sport.\",\n",
    "    \"Apples are red and sweet.\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings.shape)  # (3, 384)\n",
    "# Each sentence is now a 384-dimension vector that captures meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75b766",
   "metadata": {},
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb535c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar sentences:\n",
      "I love playing football.\n",
      "Soccer is my favorite sport.\n"
     ]
    }
   ],
   "source": [
    "# build a vector search system using FAISS. This is used in chatbots, search engines, RAG, memory, etc.\n",
    "# ‚úÖ Step 2: Use FAISS to search similar sentences\n",
    "import faiss          # ‚úÖ faiss: Facebook AI Similarity Search ‚Äì used to index and search high-dimensional vectors fast.\n",
    "import numpy as np    # ‚úÖ numpy: Needed to store and manipulate embeddings (vectors).\n",
    "\n",
    "# Convert embeddings to float32 (required by FAISS)\n",
    "embeddings = np.array(embeddings).astype(\"float32\") #üîç FAISS only works with float32 format. We convert sentence embeddings (from sentence-transformers) to float32.\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1]) #üß† Create a FAISS index that uses L2 (Euclidean) distance to measure similarity. embeddings.shape[1] = the number of dimensions per embedding (e.g., 384 or 768).\n",
    "index.add(embeddings)  # Add your sentence vectors to index. üìö We add all our sentence vectors to the index ‚Äî like storing them in memory for search.\n",
    "\n",
    "\n",
    "#querying faiss\n",
    "# Search for most similar sentence to a new query\n",
    "query = model.encode([\"I enjoy watching football\"]).astype(\"float32\") #üéØ We encode a new query sentence into a vector using the same sentence-transformers model. Convert to float32 for FAISS.\n",
    "\n",
    "D, I = index.search(query, k=2) #üîç FAISS searches and gives:\n",
    "# D: distances to the top-k closest vectors\n",
    "# I: indices of the most similar sentences (from original list)\n",
    "\n",
    "# Show the results - üó£Ô∏è We print the actual sentences corresponding to the top-k similar results.\n",
    "print(\"Most similar sentences:\")\n",
    "for idx in I[0]:\n",
    "    print(sentences[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9ef18",
   "metadata": {},
   "source": [
    "üìå Summary\n",
    "FAISS creates a fast search engine using vector similarity.\n",
    "We use sentence embeddings ‚Üí build index ‚Üí query ‚Üí get closest matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697fc28d",
   "metadata": {},
   "source": [
    "This finds the closest meaning sentence using vector similarity!\n",
    "when user queries -> get top 2 similar sentences (from faiss) like query\n",
    "üß† GOAL:\n",
    "We want to search similar sentences using semantic meaning (not keyword). This is useful in:\n",
    "\n",
    "- AI search engines\n",
    "- Chatbot memory\n",
    "- Document Q&A\n",
    "- Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64422f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "5\n",
      "[[ 4.9958229e-02  3.3321250e-02 -2.3285332e-03 ...  1.1092138e-02\n",
      "   8.0104306e-05 -6.7105673e-02]\n",
      " [-3.2004926e-02 -1.6430480e-03  2.6563013e-02 ...  3.8728736e-02\n",
      "   6.6925928e-02 -4.1541487e-02]\n",
      " [ 1.8890753e-02  3.5387490e-02 -3.0344751e-02 ...  6.5683750e-03\n",
      "   1.1034363e-01 -2.4785191e-02]\n",
      " [-2.1528115e-02  4.1045551e-03  4.4449449e-02 ... -4.9252391e-02\n",
      "   1.4991087e-03 -7.1676977e-02]\n",
      " [ 2.4371710e-02  3.2307386e-02  1.8054860e-02 ...  3.5636850e-02\n",
      "   6.0518291e-02  1.3624058e-02]]\n",
      "dist [[0.56211036 1.2136043  1.2308424 ]] [[4 0 2]]\n",
      "['Football is a great sport', 'I love playing cricket', 'Messi is the best football player']\n"
     ]
    }
   ],
   "source": [
    "# üîß Now let's turn this into a reusable search function:\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load model & encode your sentences\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sentences = [\n",
    "    \"I love playing cricket\",\n",
    "    \"Artificial Intelligence is the future\",\n",
    "    \"Messi is the best football player\",\n",
    "    \"AI will change the world\",\n",
    "    \"Football is a great sport\"\n",
    "]\n",
    "embeddings = model.encode(sentences).astype(\"float32\")\n",
    "# emb = model.encode(\"example sentence\")\n",
    "# print(len(emb))  # ‚ûù 384\n",
    "print(len(embeddings))  \n",
    "print(embeddings)\n",
    "\n",
    "# 2. Build the FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# 3. üîç Define the search function\n",
    "def semantic_search(query_text, top_k=3):\n",
    "    query_vec = model.encode([query_text]).astype(\"float32\")\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    print(\"dist\", distances, indices)\n",
    "    return [sentences[i] for i in indices[0]]\n",
    "\n",
    "# 4. ‚úÖ Try it\n",
    "print(semantic_search(\"Tell me about football\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dce12e",
   "metadata": {},
   "source": [
    "384:\n",
    "\n",
    "- The model turns each sentence into a 384-length vector of numbers\n",
    "- Pinecone must know this dimension to store/search properly\n",
    "- If you used a different model (e.g., one with 768 or 1024 dims), you‚Äôd change this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1ef5f",
   "metadata": {},
   "source": [
    "üîç Output:\n",
    "This will return 3 sentences most semantically similar to \"Tell me about football\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1678349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "# Step 1: Load .env file\n",
    "load_dotenv(dotenv_path=\".env\")  # Ensure .env is in the same folder\n",
    "\n",
    "# Step 2: Get API Key\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Optional check\n",
    "print(\"Loaded:\", bool(api_key))  # Should print True\n",
    "\n",
    "\n",
    "# ‚úÖ Set up the Pinecone client\n",
    "pc = Pinecone(api_key=api_key)  # or use os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "    # environment=\"us-east-1\"  # üåç Region like \"gcp-starter\" or \"us-west1-gcp\" When you create a Pinecone account, you also create a project in a specific region (like \"gcp-starter\" or \"us-west1-gcp\"). This region is your environment ‚Äî it tells Pinecone where to store and access your vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25495ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ STEP 3: Create Index (1-time setup)\n",
    "index_name = \"semantic-search-demo\"\n",
    "\n",
    "# ‚úÖ Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index_for_model(  #Creates a new index using a pre-defined embedding model (LLaMA v2).\n",
    "        name=index_name,    \n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\": \"llama-text-embed-v2\",     # Embedding model Pinecone will use (so you don‚Äôt need to generate embeddings manually).\n",
    "            \"field_map\": {\"text\": \"chunk_text\"} # \tTells Pinecone: ‚ÄúIn my documents, take the chunk_text field as the input text.‚Äù\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d945011",
   "metadata": {},
   "source": [
    "üí° Why it's powerful:\n",
    "You don‚Äôt need to create embeddings manually anymore ‚Äî just send chunks with a chunk_text field, and Pinecone will embed it using llama-text-embed-v2 automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pushing data to pinecone\n",
    "index = pc.Index(index_name)  # Connect to the index\n",
    "\n",
    "# Example chunks - Use upsert_records for text-based inserts:\n",
    "chunks = [ # chunk_text must match your field_map={\"text\": \"chunk_text\"} from when you created the index.\n",
    "    {\n",
    "        \"_id\": \"doc1#chunk1\",\n",
    "        \"chunk_text\": \"Pinecone is a vector database for semantic search.\",\n",
    "    },\n",
    "    {\n",
    "        \"_id\": \"doc1#chunk2\",\n",
    "        \"chunk_text\": \"It enables Retrieval-Augmented Generation (RAG) applications.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# ‚úÖ Upsert text data ‚Äî Pinecone handles embeddings\n",
    "index.upsert_records(namespace=\"__default__\", records=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38325c3d",
   "metadata": {},
   "source": [
    "# üîπ What is a Serverless Index in Pinecone?\n",
    "A serverless index is a special type of Pinecone index that:\n",
    "\n",
    "- Auto-scales ‚Äî you don‚Äôt need to manage or provision resources (no pods, no replicas).\n",
    "- Embeds text automatically ‚Äî Pinecone does the embedding for you if you select a hosted embedding model (like llama-text-embed-v2) while creating the index.\n",
    "- Is cheaper and easier to get started with.\n",
    "\n",
    "\n",
    "# üß† Why it matters for us:\n",
    "You're using:\n",
    "\n",
    "# pc.create_index_for_model(...)\n",
    "That automatically creates a serverless index with built-in embedding (integrated embedding), so you don‚Äôt need to manually embed text yourself.\n",
    "\n",
    "And:\n",
    "\n",
    "# index.query(text=\"...\")\n",
    "‚úÖ No need for sentence-transformers or any separate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b79dd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': {'hits': [{'_id': 'doc1#chunk1',\n",
      "                      '_score': 0.41501930356025696,\n",
      "                      'fields': {'chunk_text': 'Pinecone is a vector database '\n",
      "                                               'for semantic search.'}},\n",
      "                     {'_id': 'doc1#chunk2',\n",
      "                      '_score': 0.06871819496154785,\n",
      "                      'fields': {'chunk_text': 'It enables Retrieval-Augmented '\n",
      "                                               'Generation (RAG) '\n",
      "                                               'applications.'}}]},\n",
      " 'usage': {'embed_total_tokens': 10, 'read_units': 6}}\n",
      "\n",
      "Score: 0.41501930356025696\n",
      "Text: Pinecone is a vector database for semantic search.\n",
      "\n",
      "Score: 0.06871819496154785\n",
      "Text: It enables Retrieval-Augmented Generation (RAG) applications.\n"
     ]
    }
   ],
   "source": [
    "# let‚Äôs run semantic search on the data you just upserted.\n",
    "\n",
    "# üîç Search with a query string ‚Äî Pinecone auto-embeds it (because your index has embedding built-in)\n",
    "query_text = \"What is Pinecone used for?\"\n",
    "\n",
    "results = index.search(\n",
    "    namespace=\"__default__\",\n",
    "    query={\n",
    "        \"inputs\": {\"text\": query_text},  # this uses the integrated embedding -> This will be converted to an embedding using the model attached to the index.\n",
    "        \"top_k\": 3                       #  It fetches the top 3 most relevant vector matches.\n",
    "    },\n",
    "    fields=[\"chunk_text\", \"category\"]  # or any metadata fields you added\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "# üñ®Ô∏è Print matched results\n",
    "for match in results[\"result\"][\"hits\"]:\n",
    "    print(f\"\\nScore: {match['_score']}\")\n",
    "    print(f\"Text: {match['fields']['chunk_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2a430",
   "metadata": {},
   "source": [
    "now let‚Äôs move to chatbot memory using RAG-style flow.\n",
    "\n",
    "We already have:\n",
    "- üîπ Document chunks stored in Pinecone.\n",
    "- üîπ Text queries giving back semantically similar results.\n",
    "\n",
    "üß† Now we‚Äôll simulate a chatbot that:\n",
    "- Takes a user question.\n",
    "- Searches Pinecone for relevant chunks (memory).\n",
    "- Uses those chunks as context to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f08d62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 1: Function to Search Pinecone\n",
    "def search_memory(query_text, index, namespace=\"__default__\", top_k=3):\n",
    "    response = index.search(\n",
    "        namespace=namespace,\n",
    "        query={\"inputs\": {\"text\": query_text}, \"top_k\": top_k},\n",
    "        fields=[\"chunk_text\"]\n",
    "    )\n",
    "    return [hit[\"fields\"][\"chunk_text\"] for hit in response[\"result\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c772af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 2: Combine Results into Context\n",
    "def build_context(chunks):\n",
    "    return \"\\n\".join(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b73bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the context below to answer the question:\n",
      "\n",
      "Context:\n",
      "Pinecone is a vector database for semantic search.\n",
      "It enables Retrieval-Augmented Generation (RAG) applications.\n",
      "\n",
      "Question: How is Pinecone used in AI?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 3: Simulate Chatbot Answering with Context\n",
    "# We'll use a basic LLM like text-davinci-003 or chatgpt via OpenAI API ‚Äî or for local testing, just print() the prompt.\n",
    "user_question = \"How is Pinecone used in AI?\"\n",
    "\n",
    "relevant_chunks = search_memory(user_question, index)\n",
    "context = build_context(relevant_chunks)\n",
    "\n",
    "final_prompt = f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(final_prompt)  # replace this with OpenAI call later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eddb5404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Loaded: True\n",
      " Pinecone helps with semantic search by providing a vector database specifically designed for it. Semantic search is a type of search that aims to understand the intent and context behind the query, rather than just matching keywords. Pinecone enables this by using Retrieval-Augmented Generation (RAG) applications, which combine the power of machine learning models for understanding the context with the efficiency of vector databases for quick retrieval of relevant information. This allows Pinecone to deliver more accurate and meaningful search results.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")  \n",
    "\n",
    "# Check if it's loaded\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Fetch the API key securely\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key # or paste directly as string\n",
    ")\n",
    "\n",
    "print(\"API Key Loaded:\", bool(api_key))  # Optional: confirm it's loaded, not print full key\n",
    "\n",
    "# üîÆ RAG-style Answer Generator\n",
    "def generate_answer(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",  # Try Claude or GPT if needed\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# üß† RAG Flow\n",
    "user_question = \"How does Pinecone help with semantic search?\"\n",
    "relevant_chunks = search_memory(user_question, index)  # <-- define this function earlier\n",
    "context = build_context(relevant_chunks)               # <-- define this function earlier\n",
    "\n",
    "# üìù Build prompt with context\n",
    "prompt = f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# ü§ñ Get and print answer\n",
    "response = generate_answer(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2cb08",
   "metadata": {},
   "source": [
    "summary of what we built, the step-by-step flow, the purpose, and how to turn it into a real chatbot with chat history. Let‚Äôs go!\n",
    "\n",
    "# ‚úÖ WHAT WE DID\n",
    "We built a RAG-style (Retrieval-Augmented Generation) chatbot using:\n",
    "\n",
    "- üß† OpenRouter LLMs (free/alternative to OpenAI)\n",
    "- üßæ Pinecone vector DB for semantic search\n",
    "- üßë‚Äçüíª Your own context data (example chunks)\n",
    "- üì• A prompt builder to generate answers from relevant data\n",
    "\n",
    "# üéØ PURPOSE\n",
    "To answer user questions using your own data, not just what the LLM was trained on.\n",
    "\n",
    "Instead of guessing, the LLM:\n",
    "- Searches your custom docs via Pinecone (semantic search)\n",
    "- Gets relevant context\n",
    "- Answers only based on that\n",
    "\n",
    "This is how tools like ChatPDF, Notion AI, ChatGPT RAG bots work.\n",
    "\n",
    "# üîÅ FLOW WE BUILT\n",
    "# 1. Store Chunks in Pinecone\n",
    "You:\n",
    "- Break text into small chunks\n",
    "- Embed them (using an embedding model)\n",
    "- Store in Pinecone with IDs\n",
    "\n",
    "2. Search Pinecone by Semantic Similarity\n",
    "You:\n",
    "- Take user's question\n",
    "- Embed it\n",
    "- Search in Pinecone for similar chunks\n",
    "\n",
    "3. Build Context from Top Chunks\n",
    "You:\n",
    "- Extract the matched chunks' text\n",
    "- Combine into a context string\n",
    "\n",
    "4. Send Prompt to LLM\n",
    "<!-- You:\n",
    "\n",
    "Create a prompt like:\n",
    "\n",
    "text\n",
    "Copy code\n",
    "Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{relevant_chunks}\n",
    "\n",
    "Question: {user_question}\n",
    "Answer: -->\n",
    "Send to the LLM (like mistral-7b-instruct) via OpenRouter\n",
    "\n",
    "5. LLM Answers Based on Context\n",
    "You:\n",
    "Print or return the response to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773ba8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí¨ ADDING CHAT HISTORY (to make it a real chatbot)\n",
    "# Right now, each prompt is single-turn (no memory).\n",
    "# To add chat memory, do this:\n",
    "\n",
    "# ‚úÖ Modified generate_answer function:\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def generate_answer(user_input, context):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\n",
    "Answer:\"\"\"})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",  # or any model from OpenRouter\n",
    "        messages=chat_history,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    assistant_reply = response.choices[0].message.content\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "    return assistant_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dee91e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  Hello! I'm here to help. Pinecone is a vector database designed for semantic search. It supports Retrieval-Augmented Generation (RAG) applications. How can I assist you with Pinecone or RAG today?\n",
      "Bot:  Certainly! Pinecone is a vector database designed for semantic search. It enables Retrieval-Augmented Generation (RAG) applications, which are used to generate responses to user queries by combining retrieved data with pre-defined templates. This makes it a powerful tool for a variety of applications such as chatbots, content generation, and recommendation systems.\n",
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# üîÅ How to use it in a loop:\n",
    "while True:\n",
    "    user_question = input(\"You: \")\n",
    "    if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "\n",
    "    relevant_chunks = search_memory(user_question, index)\n",
    "    context = build_context(relevant_chunks)\n",
    "\n",
    "    reply = generate_answer(user_question, context)\n",
    "    print(\"Bot:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1751f",
   "metadata": {},
   "source": [
    "how to turn it into a real chatbot with chat history. Let‚Äôs go!\n",
    "\n",
    "‚úÖ WHAT WE DID\n",
    "We built a RAG-style (Retrieval-Augmented Generation) chatbot using:\n",
    "- üß† OpenRouter LLMs (free/alternative to OpenAI)\n",
    "- üßæ Pinecone vector DB for semantic search\n",
    "- üßë‚Äçüíª Your own context data (example chunks)\n",
    "- üì• A prompt builder to generate answers from relevant data\n",
    "\n",
    "üéØ PURPOSE\n",
    "- To answer user questions using your own data, not just what the LLM was trained on.\n",
    "- Instead of guessing, the LLM:\n",
    "- Searches your custom docs via Pinecone (semantic search)\n",
    "- Gets relevant context\n",
    "- Answers only based on that\n",
    "- This is how tools like ChatPDF, Notion AI, ChatGPT RAG bots work.\n",
    "\n",
    "üîÅ FLOW WE BUILT\n",
    "1. Store Chunks in Pinecone\n",
    "You:\n",
    "\n",
    "- Break text into small chunks\n",
    "- Embed them (using an embedding model)\n",
    "- Store in Pinecone with IDs\n",
    "\n",
    "2. Search Pinecone by Semantic Similarity\n",
    "You:\n",
    "\n",
    "Take user's question\n",
    "- Embed it\n",
    "- Search in Pinecone for similar chunks\n",
    "\n",
    "3. Build Context from Top Chunks\n",
    "You:\n",
    "\n",
    "- Extract the matched chunks' text\n",
    "- Combine into a context string\n",
    "\n",
    "4. Send Prompt to LLM\n",
    "You:\n",
    "\n",
    "- Create a prompt\n",
    "- Send to the LLM (like mistral-7b-instruct) via OpenRouter\n",
    "\n",
    "5. LLM Answers Based on Context\n",
    "You:\n",
    "\n",
    "- Print or return the response to the user\n",
    "- üí¨ ADDING CHAT HISTORY (to make it a real chatbot)\n",
    "- Right now, each prompt is single-turn (no memory).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6b5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# //using widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3167e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 2: Setup Chat History + Functions\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# Chat handler\n",
    "def generate_answer(user_input, context):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\n",
    "Answer:\"\"\"})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",\n",
    "        messages=chat_history,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162895dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 3: Create Input/Output Widgets\n",
    "input_box = widgets.Text(\n",
    "    placeholder='Type your question here...',\n",
    "    description='You:',\n",
    "    layout=widgets.Layout(width='90%')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a0c8508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/813pxpzx7nld35mftk3q49jw0000gp/T/ipykernel_53318/1436709783.py:17: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff08a4be1a214091a91f1a83cf41aed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='You:', layout=Layout(width='90%'), placeholder='Type your question here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc55f65db3ec4f0a873797fe07662ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚úÖ Step 4: Create Callback + Display UI\n",
    "def on_submit(change):\n",
    "    user_q = input_box.value\n",
    "    input_box.value = ''  # Clear input\n",
    "\n",
    "    with output_area:\n",
    "        # üîç RAG: Search + Context\n",
    "        relevant_chunks = search_memory(user_q, index)\n",
    "        context = build_context(relevant_chunks)\n",
    "\n",
    "        # ü§ñ Get Answer\n",
    "        reply = generate_answer(user_q, context)\n",
    "        print(f\"\\nYou: {user_q}\")\n",
    "        print(f\"Bot: {reply}\")\n",
    "\n",
    "# ‚úÖ Attach Submit Event\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "# ‚úÖ Show UI\n",
    "display(input_box, output_area)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acad2af",
   "metadata": {},
   "source": [
    "Let‚Äôs add PDF support into your RAG chatbot before turning it into a web app. Here's the plan:\n",
    "\n",
    "‚úÖ PDF Ingestion Plan (Phase 4: RAG with Real Docs)\n",
    "üîπ Step 1: Install Dependencies\n",
    "\n",
    "üîπ Step 2: Extract Text from PDF\n",
    "We'll use PyPDF2 or unstructured (for more accurate parsing).\n",
    "\n",
    "üîπ Step 3: Chunk the Text\n",
    "Split the large PDF text into smaller chunks (e.g., 200‚Äì300 words each).\n",
    "\n",
    "üîπ Step 4: Push to Pinecone\n",
    "Each chunk will be upserted with a unique _id and chunk_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acc7e24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966c6a68df3840b892784c3102a11909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚úÖ Step 2: Upload + Extract Text from PDF\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "\n",
    "# üîº Upload PDF using file upload widget\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "upload_btn = widgets.FileUpload(accept='.pdf', multiple=False)\n",
    "display(upload_btn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43408be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a61a00105241ffb22ac58022b44e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üì• Step 1: Upload widget\n",
    "upload_btn = widgets.FileUpload(accept='.pdf', multiple=False)\n",
    "display(upload_btn)\n",
    "\n",
    "# üì§ Step 2: Extract after file is uploaded\n",
    "def extract_pdf_text(file_upload_widget):\n",
    "    if file_upload_widget.value:\n",
    "        # ‚úÖ Extract the first uploaded file from the tuple\n",
    "        uploaded_file = file_upload_widget.value[0]\n",
    "        content = uploaded_file['content']\n",
    "        reader = PdfReader(BytesIO(content))\n",
    "\n",
    "        # ‚úÖ Extract text from all pages\n",
    "        text = \"\\n\".join(page.extract_text() for page in reader.pages)\n",
    "        return text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3741097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rakshath U Shetty\n",
      "Bengaluru, India | +91-8951085835 | rakshathushettyu6@gmail.com | LinkedIn | Github | Portfolio\n",
      "EDUCATION\n",
      "Dayananda Sagar University Bengaluru, India\n",
      "Bachelor of Technology in Computer Science. (SGPA:10 CGPA: 9.31) Graduation Date: May 2024\n",
      "EXPERIENCE\n",
      "Startup Role: Software Engineer Intern Jul 2024 - Dec 2024\n",
      "Freo - MoneyTap| Backend Java Developer Bengaluru, India\n",
      "‚Ä¢ Developed and debugged code, leveraging log analysis to resolve issues and maintain system stability; written SDK test scripts\n",
      "in REST to automate FinFlux testing, reducing manual testing time by 40% and improving deployment reliability by 25%.\n",
      "‚Ä¢ Directed the repayments service, owning the process in a 6-member backend team, automating updates for customer\n",
      "collections from lending partners, leading to a 30% efficiency gain and 20% improvement in payment accuracy.\n",
      "Startup Role: Full Stack Developer, Junior Data Engineer Sept 2023 - Feb 2024\n",
      "AnTech Crew - Freelancing Startup Bengaluru, India\n",
      "‚Ä¢ Pioneered the\n"
     ]
    }
   ],
   "source": [
    "pdf_text = extract_pdf_text(upload_btn)\n",
    "print(pdf_text[:1000])  # preview # first 1000 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f11b585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 21\n",
      "['Rakshath U Shetty\\nBengaluru, India | +91-8951085835 | rakshathushettyu6@gmail.com | LinkedIn | Github | Portfolio\\nEDUCATION\\nDayananda Sagar University Bengaluru, India\\nBachelor of Technology in Computer Science. (SGPA:10 CGPA: 9.31) Graduation Date: May 2024\\nEXPERIENCE\\nStartup Role: Software Enginee', 'May 2024\\nEXPERIENCE\\nStartup Role: Software Engineer Intern Jul 2024 - Dec 2024\\nFreo - MoneyTap| Backend Java Developer Bengaluru, India\\n‚Ä¢ Developed and debugged code, leveraging log analysis to resolve issues and maintain system stability; written SDK test scripts\\nin REST to automate FinFlux testing']\n"
     ]
    }
   ],
   "source": [
    "# let's chunk the PDF text and push it to Pinecone using the same logic we used earlier.\n",
    "# ‚úÖ Step 1: Split PDF Text into Chunks\n",
    "from textwrap import wrap\n",
    "\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "pdf_chunks = chunk_text(pdf_text)\n",
    "print(f\"Total chunks: {len(pdf_chunks)}\")\n",
    "print(pdf_chunks[:2])  # Preview first 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d305e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 2: Format and Push to Pinecone\n",
    "# We assume your Pinecone index was already created with this setup:\n",
    "# index_name = \"semantic-search-demo\"\n",
    "# index = pc.Index(index_name)\n",
    "\n",
    "records = []\n",
    "for i, chunk in enumerate(pdf_chunks):\n",
    "    records.append({\n",
    "        \"_id\": f\"pdf#chunk{i}\",\n",
    "        \"chunk_text\": chunk\n",
    "    })\n",
    "\n",
    "index.upsert_records(namespace=\"pdf-doc\", records=records)\n",
    "print(\"‚úÖ Uploaded to Pinecone!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee64aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Loaded: True\n",
      " Rakshath U Shetty has worked on two notable projects as per the provided context:\n",
      "\n",
      "1. UR-Connect: This was a project that Rakshath led his team to participate in and place in the top 2 at a highly competitive State Level Project Competition held at BIT - Bengaluru Institute of Technology. The competition had over 300+ participants.\n",
      "\n",
      "2. Code 360: While working as a Software Engineer at a startup, Rakshath participated in Code 360, where he demonstrated exceptional skills and resolved over 1000 complex problems. He also achieved the Grand Master, Dominator (highest rank among 7 leagues), and MASTER titles in this competition.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")  \n",
    "\n",
    "# Check if it's loaded\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Fetch the API key securely\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key # or paste directly as string\n",
    ")\n",
    "\n",
    "print(\"API Key Loaded:\", bool(api_key))  # Optional: confirm it's loaded, not print full key\n",
    "\n",
    "# üîÆ RAG-style Answer Generator\n",
    "def generate_answer(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",  # Try Claude or GPT if needed\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# üß† RAG Flow\n",
    "user_question = \"what projects rakshath worked on?\"\n",
    "relevant_chunks = search_memory(user_question, index, namespace=\"pdf-doc\")  # <-- define this function earlier\n",
    "context = build_context(relevant_chunks)               # <-- define this function earlier\n",
    "\n",
    "# üìù Build prompt with context\n",
    "prompt = f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# ü§ñ Get and print answer\n",
    "response = generate_answer(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7360a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ PDF Q&A Inside while Loop\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_question = input(\"You: \")\n",
    "    if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "\n",
    "    # 1. Search Pinecone for relevant PDF chunks (namespace=\"pdf-doc\")\n",
    "    relevant_chunks = search_memory(user_question, index, namespace=\"pdf-doc\")\n",
    "\n",
    "    # 2. Build the context from those chunks\n",
    "    context = build_context(relevant_chunks)\n",
    "\n",
    "    # 3. Append question to chat history\n",
    "    chat_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_question}\n",
    "Answer:\"\"\"\n",
    "    })\n",
    "\n",
    "    # 4. Get the LLM-generated response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",\n",
    "        messages=chat_history,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    # 5. Extract and display reply\n",
    "    assistant_reply = response.choices[0].message.content\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "    print(\"Bot:\", assistant_reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5d6e4",
   "metadata": {},
   "source": [
    "‚úÖ Step 1: Ask a Question\n",
    "Just type a question like:\n",
    "\n",
    "- \"What projects has Rakshath worked on?\"\n",
    "- \"What skills does this resume highlight?\"\n",
    "- \"Tell me about Rakshath‚Äôs work experience?\"\n",
    "\n",
    "üß† How It Works (Quick Recap):\n",
    "We search your uploaded resume chunks from pdf-doc namespace in Pinecone.\n",
    "\n",
    "- Grab top matches based on semantic similarity.\n",
    "- Pass them to the LLM via OpenRouter.\n",
    "- LLM answers only from your resume (RAG-style).\n",
    "\n",
    "\n",
    "Here's the exact questioning logic used in our RAG-style PDF chatbot (semantic search + OpenRouter LLM):\n",
    "\n",
    "üß† Why This Works\n",
    "- We use semantic search to find only relevant info from your PDF.\n",
    "- Then let the LLM generate an answer based only on that (RAG).\n",
    "- Ensures the bot answers from your data, not from its training.\n",
    "\n",
    "‚úÖ You can now reuse this for PDFs, Docs, YouTube transcripts, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b8e50",
   "metadata": {},
   "source": [
    "‚úÖ What You‚Äôve Completed:\n",
    "You now have a full pipeline for:\n",
    "- üìÑ Uploading a PDF\n",
    "- üß† Extracting and chunking the text\n",
    "- üîç Indexing into Pinecone\n",
    "- ü§ñ Asking questions using semantic search + LLM (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 12:45:08.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.729 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.730 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.730 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.732 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-30 12:45:08.732 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Streamlit Web App: PDF RAG Chatbot\n",
    "\n",
    "import streamlit as st\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" #disabling parallel processing\n",
    "\n",
    "\n",
    "# Load .env for OpenRouter API\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=api_key)\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"üìÑ Chat with Your PDF (RAG + Pinecone style)\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    reader = PdfReader(uploaded_file)\n",
    "    raw_text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "    # Split into chunks\n",
    "    def chunk_text(text, chunk_size=300):\n",
    "        words = text.split()\n",
    "        return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "    chunks = chunk_text(raw_text)\n",
    "    embeddings = model.encode(chunks).astype(\"float32\")\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Chat history\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "        ]\n",
    "\n",
    "    # Search top-k relevant chunks\n",
    "    def search_context(query, top_k=3):\n",
    "        query_vec = model.encode([query]).astype(\"float32\")\n",
    "        _, I = index.search(query_vec, top_k)\n",
    "        return [chunks[i] for i in I[0]]\n",
    "\n",
    "    # Generate answer\n",
    "    def generate_answer(user_q):\n",
    "        context = \"\\n\".join(search_context(user_q))\n",
    "        prompt = f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_q}\n",
    "Answer:\"\"\"\n",
    "        st.session_state.chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"mistralai/mistral-7b-instruct\",\n",
    "            messages=st.session_state.chat_history,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "    # Input box\n",
    "    user_input = st.text_input(\"Ask a question about the PDF\")\n",
    "    if user_input:\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            answer = generate_answer(user_input)\n",
    "            st.markdown(f\"**You:** {user_input}\")\n",
    "            st.markdown(f\"**Bot:** {answer}\")\n",
    "\n",
    "\n",
    "# echo 'export PATH=$PATH:~/Library/Python/3.9/bin' >> ~/.zshrc\n",
    "# source ~/.zshrc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8548afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Streamlit Web App: PDF RAG Chatbot (Fixed Version)\n",
    "code = \"\"\"\n",
    "import streamlit as st\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # disabling parallel processing\n",
    "\n",
    "# Load .env for OpenRouter API\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=api_key)\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"üìÑ Chat with Your PDF (RAG + FAISS style)\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    reader = PdfReader(uploaded_file)\n",
    "    raw_text = \"\\\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "    # Split into chunks\n",
    "    def chunk_text(text, chunk_size=300):\n",
    "        words = text.split()\n",
    "        return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "    chunks = chunk_text(raw_text)\n",
    "    embeddings = model.encode(chunks).astype(\"float32\")\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Chat history\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "        ]\n",
    "\n",
    "    # Search top-k relevant chunks\n",
    "    def search_context(query, top_k=3):\n",
    "        query_vec = model.encode([query]).astype(\"float32\")\n",
    "        _, I = index.search(query_vec, top_k)\n",
    "        return [chunks[i] for i in I[0]]\n",
    "\n",
    "    # Generate answer\n",
    "    def generate_answer(user_q):\n",
    "        context = \"\\\\n\".join(search_context(user_q))\n",
    "        prompt = (\n",
    "            \"Use the context below to answer the question:\\\\n\\\\n\"\n",
    "            f\"Context:\\\\n{context}\\\\n\\\\n\"\n",
    "            f\"Question: {user_q}\\\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        st.session_state.chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"mistralai/mistral-7b-instruct\",\n",
    "            messages=st.session_state.chat_history,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "    # Input box\n",
    "    user_input = st.text_input(\"Ask a question about the PDF\")\n",
    "    if user_input:\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            answer = generate_answer(user_input)\n",
    "            st.markdown(f\"**You:** {user_input}\")\n",
    "            st.markdown(f\"**Bot:** {answer}\")\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"chat_pdf_app.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "#streamlit run chat_pdf_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec067e56",
   "metadata": {},
   "source": [
    " Why You See It\n",
    "Hugging Face's tokenizers library supports parallel processing (multi-threading).\n",
    "\n",
    "But when you load tokenizers inside a web app (like Streamlit) that uses multiprocessing or forking, Python gets confused and disables parallelism to avoid crashes.\n",
    "\n",
    "It shows this warning to inform you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

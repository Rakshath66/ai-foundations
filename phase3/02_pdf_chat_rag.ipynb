{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fed9046",
   "metadata": {},
   "source": [
    "# ‚úÖ Streamlit Web App: PDF RAG Chatbot ‚Äî Debugging + Problem-Solving Summary\n",
    "\n",
    "Here's a detailed breakdown of every issue faced during development of your PDF RAG chatbot using Streamlit, SentenceTransformers, FAISS, and OpenRouter.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Project Purpose\n",
    "You built a chatbot that can:\n",
    "1. Accept a PDF upload\n",
    "2. Chunk and embed the text using SentenceTransformers\n",
    "3. Use FAISS for similarity search\n",
    "4. Send top chunks to Mistral-7B via OpenRouter\n",
    "5. Display chat UI with stylized messages\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Problems & Detailed Solutions\n",
    "\n",
    "### 1. üö® SentenceTransformer Meta Tensor Error\n",
    "**Issue:**\n",
    "On Hugging Face Spaces, this line crashed:\n",
    "```python\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "```\n",
    "**Error:** `NotImplementedError: Cannot copy out of meta tensor`\n",
    "\n",
    "**Cause:** HF Spaces uses containerized environments with strict memory and device permissions. Torch tried to move a model from `meta` device to `cpu`, which is not permitted without setting the cache and home directories correctly.\n",
    "\n",
    "**Solution:**\n",
    "Set environment variables to use `/tmp` instead of system-level directories:\n",
    "```python\n",
    "import tempfile, platform\n",
    "base_cache = \"/tmp\"\n",
    "os.environ[\"HF_HOME\"] = base_cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = base_cache\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = base_cache\n",
    "```\n",
    "‚úÖ You resolved it by isolating model cache to temp space ‚Äî preventing write conflicts.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üõë Streamlit Permission Errors in Deployment\n",
    "**Issue:**\n",
    "Streamlit tried to write to a system directory like `~/.streamlit`, but HF containers don‚Äôt allow it.\n",
    "\n",
    "**Symptoms:**\n",
    "- `PermissionError` when initializing Streamlit\n",
    "- No session state retained\n",
    "\n",
    "**Solution:**\n",
    "Redirect config to `/tmp`:\n",
    "```python\n",
    "os.environ[\"STREAMLIT_CONFIG_DIR\"] = os.path.join(base_cache, \".streamlit\")\n",
    "```\n",
    "‚úÖ You prevented permission issues and got smooth deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üì± Mobile Upload Doesn‚Äôt Work\n",
    "**Issue:** File upload worked in desktop but not on mobile browser\n",
    "\n",
    "**Debugging Steps:**\n",
    "- Tried mobile Chrome, Android devices\n",
    "- Used DevTools ‚Üí Toggle device toolbar\n",
    "\n",
    "**Solution:**\n",
    "Verified Streamlit works fine after full deploy; tested on real device and mobile emulator. Issue was local-only.\n",
    "\n",
    "‚úÖ Confirmed deployment works across devices.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. üê¢ Slow Mistral API Response\n",
    "**Issue:** Long delay from OpenRouter API when full PDF chunks sent\n",
    "\n",
    "**Solution:**\n",
    "- Limited to **top 3** FAISS matches only\n",
    "- Reduced temperature to 0.3 for focused replies\n",
    "```python\n",
    "def semantic_retrieve(query, k=3):\n",
    "    ...\n",
    "```\n",
    "‚úÖ This gave faster, concise answers.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. üé® Streamlit CSS Doesn‚Äôt Apply to st.form\n",
    "**Issue:** CSS styles like buttons, input text field, chat bubbles didn‚Äôt apply inside `st.form()`\n",
    "\n",
    "**Why:** Streamlit wraps elements in their own divs/classes and overrides inline styles.\n",
    "\n",
    "**Fix:**\n",
    "Used custom `div` + `unsafe_allow_html=True`:\n",
    "```python\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".user-msg { background: #ffe259; ... }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "```\n",
    "‚úÖ You styled messages and buttons by directly injecting CSS.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ü§ñ No Chat History Retention\n",
    "**Issue:** Model lost context every time user asked a new question.\n",
    "\n",
    "**Solution:**\n",
    "Stored past messages in `st.session_state`:\n",
    "```python\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "```\n",
    "‚úÖ Chat now remembers the past.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. üîí API Key Not Loading\n",
    "**Issue:** Local .env worked fine, but not on HF Spaces\n",
    "\n",
    "**Fix:**\n",
    "Avoided `load_dotenv()` for production ‚Äî instead, used:\n",
    "```python\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "```\n",
    "‚úÖ Let Hugging Face Secrets handle the secure key.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. üåÄ Spinner Boring ‚Üí Custom Loader\n",
    "**Issue:** Default spinner was too plain\n",
    "\n",
    "**Fix:** Injected a CSS loader:\n",
    "```html\n",
    ".loader {\n",
    "  border: 6px solid #f3f3f3;\n",
    "  border-top: 6px solid #ffa751;\n",
    "  animation: spin 1s linear infinite;\n",
    "}\n",
    "```\n",
    "‚úÖ Looks more alive and engaging\n",
    "\n",
    "---\n",
    "\n",
    "### 9. ‚ú® Chat UI Looks Bland\n",
    "**Issue:** User & bot messages looked the same\n",
    "\n",
    "**Fix:**\n",
    "Added `.user-msg` and `.bot-msg` divs:\n",
    "```html\n",
    "<div class=\"user-msg\">Hello!</div>\n",
    "<div class=\"bot-msg\">Hi there!</div>\n",
    "```\n",
    "Styled with gradients, padding, borders, etc.\n",
    "‚úÖ Chat now looks like a real conversation!\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Debugging Skills You Used\n",
    "| Tool/Method | Usage |\n",
    "|-------------|--------|\n",
    "| `st.write()` / print | To debug session state, inputs, API keys |\n",
    "| DevTools (F12) | Tested mobile responsiveness, CSS injection |\n",
    "| Browser logs | Found missing keys, model issues |\n",
    "| Manual testing | On real Android device, to confirm UX |\n",
    "| Trial & Error | Gradually disabled/re-enabled sections |\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Result\n",
    "- ‚úÖ Deployed successfully on Hugging Face Spaces\n",
    "- üí¨ Styled UI with chat bubbles, color themes\n",
    "- üîê Secure OpenRouter integration\n",
    "- üîç PDF chunking + semantic search\n",
    "- üîó Full working end-to-end chatbot\n",
    "\n",
    "Let me know if you want this turned into a project blog, README, or PDF summary!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7e57e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ef7770e",
   "metadata": {},
   "source": [
    "Great ‚Äî here‚Äôs an **in-depth explanation of the 4 critical problems** you faced in your PDF RAG chatbot project, along with the **code solutions** and **debugging strategies**:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. üõë Model Loading Failure in Hugging Face Spaces**\n",
    "\n",
    "#### ‚úÖ Problem:\n",
    "\n",
    "When loading the SentenceTransformer model:\n",
    "\n",
    "```python\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "you got this error:\n",
    "\n",
    "```\n",
    "NotImplementedError: Cannot copy out of meta tensor; no data!\n",
    "```\n",
    "\n",
    "#### üß† Why it happened:\n",
    "\n",
    "Hugging Face Spaces containers have limited permissions and restricted memory allocation. When PyTorch tries to load a model, it sometimes uses `meta tensors` (placeholders without data). Then it fails to move them to CPU/GPU because the container blocks that memory access.\n",
    "\n",
    "#### ‚úÖ Fix:\n",
    "\n",
    "Redirect all Hugging Face + PyTorch cache directories to `/tmp`, a safe writeable space in cloud containers.\n",
    "\n",
    "#### ‚úÖ Code Fix:\n",
    "\n",
    "```python\n",
    "import tempfile, platform\n",
    "base_cache = \"/tmp\"  # temp folder on Linux\n",
    "\n",
    "os.environ[\"HF_HOME\"] = base_cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = base_cache\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = base_cache\n",
    "```\n",
    "\n",
    "#### üîç Debugging Method:\n",
    "\n",
    "* Compared **local success** vs **remote failure**\n",
    "* Read Hugging Face container permissions docs\n",
    "* Searched forums for ‚Äúmeta tensor‚Äù + ‚ÄúHugging Face Spaces‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### **2. üìÅ Streamlit Fails to Write to /tmp**\n",
    "\n",
    "#### ‚úÖ Problem:\n",
    "\n",
    "You got a `PermissionError` for:\n",
    "\n",
    "```\n",
    "/home/user/.streamlit/machine_id_v4\n",
    "```\n",
    "\n",
    "Streamlit tried writing telemetry data in the default home dir, which isn't writable in Spaces.\n",
    "\n",
    "#### ‚úÖ Fix:\n",
    "\n",
    "Redirect Streamlit's config to a safe temp location:\n",
    "\n",
    "```python\n",
    "os.environ[\"STREAMLIT_CONFIG_DIR\"] = os.path.join(base_cache, \".streamlit\")\n",
    "```\n",
    "\n",
    "#### üîç Debugging Method:\n",
    "\n",
    "* Read the **full error traceback**\n",
    "* Noticed it tried writing to `.streamlit/machine_id_v4`\n",
    "* Googled ‚Äústreamlit config permission Hugging Face‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### **3. üì§ File Uploader Not Working on Mobile**\n",
    "\n",
    "#### ‚úÖ Problem:\n",
    "\n",
    "File upload worked on desktop, but mobile Chrome wouldn‚Äôt open file selector or failed to respond.\n",
    "\n",
    "#### ‚úÖ Fix:\n",
    "\n",
    "* Fully deployed the app\n",
    "* Ensured `accept=\".pdf\"` was correct\n",
    "* Used Chrome DevTools ‚Üí Mobile Emulation\n",
    "* Also tested on **real Android device**\n",
    "\n",
    "```python\n",
    "uploaded = st.file_uploader(\"Upload a PDF\", type=\"pdf\")\n",
    "```\n",
    "\n",
    "#### üîç Debugging Method:\n",
    "\n",
    "* Used browser dev tools: ‚úÖ emulate mobile, ‚úÖ inspect input tag\n",
    "* Deployed and tested on real device (Samsung/Pixel)\n",
    "* Realized it was just a **local access limitation** ‚Äî not code issue.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. üìà Performance Issues on Mistral Model**\n",
    "\n",
    "#### ‚úÖ Problem:\n",
    "\n",
    "When sending full prompt with large context, Mistral via OpenRouter responded slowly or timed out.\n",
    "\n",
    "#### ‚úÖ Fix:\n",
    "\n",
    "* Limited FAISS search to **top-3 chunks**\n",
    "* Reduced `temperature` to **0.3** for shorter, focused responses\n",
    "\n",
    "```python\n",
    "def semantic_retrieve(query, k=3):\n",
    "    q_vec = model.encode([query]).astype(\"float32\")\n",
    "    _, I = index.search(q_vec, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/mistral-7b-instruct\",\n",
    "    messages=st.session_state.chat_history,\n",
    "    temperature=0.3\n",
    ")\n",
    "```\n",
    "\n",
    "#### üîç Debugging Method:\n",
    "\n",
    "* Logged API latency using `st.spinner()` timing\n",
    "* Removed long context manually\n",
    "* Compared Mistral vs GPT output lengths\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to turn this into a `README` or blog-style writeup next?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907e09b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3b68d9",
   "metadata": {},
   "source": [
    "Let‚Äôs break it all down clearly:\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **What is `/tmp`?**\n",
    "\n",
    "`/tmp` is a **temporary directory** in Unix/Linux systems (like Hugging Face Spaces).\n",
    "\n",
    "* It's writable by default.\n",
    "* Files stored here may be deleted automatically after a reboot or timeout.\n",
    "* Used to **store temporary files** that an app needs at runtime (e.g., model weights, cache files, session data).\n",
    "\n",
    "üí° Hugging Face Spaces doesn‚Äôt allow writing to system directories like `/home/user` ‚Äî so `/tmp` is your safe zone.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **What is `/home/user/.streamlit/machine_id_v4`?**\n",
    "\n",
    "This is the path where Streamlit tries to **store a unique machine ID**, used for:\n",
    "\n",
    "* Anonymous usage statistics\n",
    "* Tracking session/config across runs\n",
    "\n",
    "But in **cloud containers** like Hugging Face, writing to `/home/user/.streamlit/` often throws a `PermissionError`.\n",
    "\n",
    "üîß **Fix**: We redirect that config folder to a writable location:\n",
    "\n",
    "```python\n",
    "os.environ[\"STREAMLIT_CONFIG_DIR\"] = \"/tmp/.streamlit\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What do these mean?\n",
    "\n",
    "```python\n",
    "os.environ[\"HF_HOME\"] = base_cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = base_cache\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = base_cache\n",
    "```\n",
    "\n",
    "Each of these **sets a safe location for temporary storage** of models and data:\n",
    "\n",
    "| ENV VAR                      | What it controls                                               |\n",
    "| ---------------------------- | -------------------------------------------------------------- |\n",
    "| `HF_HOME`                    | Root dir for Hugging Face files (auth, downloads, logs)        |\n",
    "| `TRANSFORMERS_CACHE`         | Cache dir for transformer model files (`*.bin`, configs, etc.) |\n",
    "| `SENTENCE_TRANSFORMERS_HOME` | SentenceTransformer model + tokenizer cache                    |\n",
    "\n",
    "üõ°Ô∏è **Why use them?**\n",
    "To **avoid permission issues** in containers like Hugging Face Spaces, which can‚Äôt write to the default cache locations like:\n",
    "\n",
    "```\n",
    "~/.cache/huggingface/\n",
    "~/.cache/torch/\n",
    "```\n",
    "\n",
    "By setting them to `/tmp`, your app can:\n",
    "\n",
    "* Load models safely\n",
    "* Write/download cache\n",
    "* Avoid runtime errors\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Would you like a diagram showing this file flow?\n",
    "Let‚Äôs break it all down clearly:\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **What is `/tmp`?**\n",
    "\n",
    "`/tmp` is a **temporary directory** in Unix/Linux systems (like Hugging Face Spaces).\n",
    "\n",
    "* It's writable by default.\n",
    "* Files stored here may be deleted automatically after a reboot or timeout.\n",
    "* Used to **store temporary files** that an app needs at runtime (e.g., model weights, cache files, session data).\n",
    "\n",
    "üí° Hugging Face Spaces doesn‚Äôt allow writing to system directories like `/home/user` ‚Äî so `/tmp` is your safe zone.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **What is `/home/user/.streamlit/machine_id_v4`?**\n",
    "\n",
    "This is the path where Streamlit tries to **store a unique machine ID**, used for:\n",
    "\n",
    "* Anonymous usage statistics\n",
    "* Tracking session/config across runs\n",
    "\n",
    "But in **cloud containers** like Hugging Face, writing to `/home/user/.streamlit/` often throws a `PermissionError`.\n",
    "\n",
    "üîß **Fix**: We redirect that config folder to a writable location:\n",
    "\n",
    "```python\n",
    "os.environ[\"STREAMLIT_CONFIG_DIR\"] = \"/tmp/.streamlit\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What do these mean?\n",
    "\n",
    "```python\n",
    "os.environ[\"HF_HOME\"] = base_cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = base_cache\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = base_cache\n",
    "```\n",
    "\n",
    "Each of these **sets a safe location for temporary storage** of models and data:\n",
    "\n",
    "| ENV VAR                      | What it controls                                               |\n",
    "| ---------------------------- | -------------------------------------------------------------- |\n",
    "| `HF_HOME`                    | Root dir for Hugging Face files (auth, downloads, logs)        |\n",
    "| `TRANSFORMERS_CACHE`         | Cache dir for transformer model files (`*.bin`, configs, etc.) |\n",
    "| `SENTENCE_TRANSFORMERS_HOME` | SentenceTransformer model + tokenizer cache                    |\n",
    "\n",
    "üõ°Ô∏è **Why use them?**\n",
    "To **avoid permission issues** in containers like Hugging Face Spaces, which can‚Äôt write to the default cache locations like:\n",
    "\n",
    "```\n",
    "~/.cache/huggingface/\n",
    "~/.cache/torch/\n",
    "```\n",
    "\n",
    "By setting them to `/tmp`, your app can:\n",
    "\n",
    "* Load models safely\n",
    "* Write/download cache\n",
    "* Avoid runtime errors\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Would you like a diagram showing this file flow?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a35cba",
   "metadata": {},
   "source": [
    "Great! Here's an in-depth explanation of each section of your **Dockerfile** and the corresponding environment/configuration decisions made in your **Streamlit PDF RAG Chatbot** project. This will help you clearly understand:\n",
    "\n",
    "---\n",
    "\n",
    "### üê≥ **Dockerfile Explanation ‚Äì Line by Line**\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Use a lightweight Python 3.9 image (slim = minimal size, faster build, less junk).\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "COPY .streamlit/ /tmp/.streamlit/\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Copies your custom Streamlit config (`config.toml`) to `/tmp/.streamlit` because `/tmp` is a writable path on Hugging Face Spaces.\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "ENV HOME=/tmp\n",
    "ENV STREAMLIT_CONFIG_DIR=/tmp/.streamlit\n",
    "ENV TOKENIZERS_PARALLELISM=false\n",
    "ENV SENTENCE_TRANSFORMERS_HOME=/tmp\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Set environment variables:\n",
    "\n",
    "* `HOME`: Ensures user-specific data is written inside `/tmp`.\n",
    "* `STREAMLIT_CONFIG_DIR`: Avoid Streamlit errors from `~/.streamlit` (which is read-only in HF containers).\n",
    "* `TOKENIZERS_PARALLELISM`: Avoids multiprocessing errors in tokenizers.\n",
    "* `SENTENCE_TRANSFORMERS_HOME`: Forces Hugging Face downloads into a writable `/tmp`.\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "WORKDIR /app\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Sets `/app` as the current working directory (best practice to isolate app logic).\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    curl \\\n",
    "    git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Installs system dependencies:\n",
    "\n",
    "* `build-essential`: Needed for Python packages with C/C++ extensions.\n",
    "* `curl`, `git`: Useful for debugging or code cloning if needed.\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "ENV TOKENIZERS_PARALLELISM=false \\\n",
    "    HF_HOME=/tmp \\\n",
    "    TRANSFORMERS_CACHE=/tmp \\\n",
    "    SENTENCE_TRANSFORMERS_HOME=/tmp\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Again sets HF-related caches to avoid read-only FS errors during model download.\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "COPY requirements.txt ./\n",
    "COPY src/ ./src/\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Copies dependencies and your app code.\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Installs Python libraries. `--no-cache-dir` avoids filling up container with pip cache (space-saving).\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "EXPOSE 8501\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Opens port 8501 ‚Äî default for Streamlit apps.\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health || exit 1\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Adds a health check. If the Streamlit app isn‚Äôt reachable on startup, container will be marked unhealthy.\n",
    "\n",
    "---\n",
    "\n",
    "```dockerfile\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"src/streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
    "```\n",
    "\n",
    "‚úîÔ∏è This runs your Streamlit app, binding it to `0.0.0.0` so it's accessible from outside the container.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why These Were Needed in Hugging Face Spaces\n",
    "\n",
    "* HF Spaces runs on secure containers with **read-only home directories** (`/home/user`). So, you redirected all temp/cache/data configs to `/tmp`.\n",
    "* Without these env vars, model downloads (`sentence_transformers`) fail with `meta-tensor` or file permission errors.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want me to go line-by-line on the `streamlit_app.py` next or explain which parts are specific to Hugging Face Spaces vs local testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a87092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19394da1",
   "metadata": {},
   "source": [
    "Let‚Äôs continue with **line-by-line explanations** of your `Dockerfile`, as you requested.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `Dockerfile` Breakdown (continued from previous)\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `FROM python:3.9-slim`\n",
    "\n",
    "* **What it does**: Sets the base image for the container ‚Äî a minimal Python 3.9 Linux image.\n",
    "* **Why**: It ensures a clean and lightweight environment with Python installed, good for deploying Python apps.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `COPY .streamlit/ /tmp/.streamlit/`\n",
    "\n",
    "* **What it does**: Copies your local `.streamlit` config folder (like `config.toml`) into the container at `/tmp/.streamlit/`.\n",
    "* **Why**: Needed for setting UI options and preventing permission errors with Hugging Face Spaces.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `ENV HOME=/tmp`\n",
    "\n",
    "* **What it does**: Sets the `$HOME` environment variable to `/tmp`.\n",
    "* **Why**: Avoids permission issues when libraries like Streamlit try to write user files.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. `ENV STREAMLIT_CONFIG_DIR=/tmp/.streamlit`\n",
    "\n",
    "* **What it does**: Forces Streamlit to read config from `/tmp/.streamlit`.\n",
    "* **Why**: Useful because default home (`/root`) may not be writable on cloud platforms like Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. `ENV TOKENIZERS_PARALLELISM=false`\n",
    "\n",
    "* **What it does**: Disables parallel tokenizers to reduce RAM usage and avoid warnings from Hugging Face tokenizers.\n",
    "* **Why**: Stable performance in limited-resource environments.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. `ENV SENTENCE_TRANSFORMERS_HOME=/tmp`\n",
    "\n",
    "* **What it does**: Tells the embedding model to cache its files in `/tmp`.\n",
    "* **Why**: Prevents `meta tensor` errors from read-only or unavailable default cache paths in cloud.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. `WORKDIR /app`\n",
    "\n",
    "* **What it does**: Sets the working directory to `/app`.\n",
    "* **Why**: All future commands will be executed from here (e.g., copying code, running app).\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. `RUN apt-get update && apt-get install -y \\ build-essential \\ curl \\ git \\ && rm -rf /var/lib/apt/lists/*`\n",
    "\n",
    "* **What it does**:\n",
    "\n",
    "  * Updates apt cache.\n",
    "  * Installs dev tools like `build-essential`, `curl`, and `git`.\n",
    "  * Cleans up apt cache to reduce image size.\n",
    "* **Why**: Required for Python wheels and some pip packages to build successfully.\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. Repeating ENV block:\n",
    "\n",
    "```dockerfile\n",
    "ENV TOKENIZERS_PARALLELISM=false \\\n",
    "    HF_HOME=/tmp \\\n",
    "    TRANSFORMERS_CACHE=/tmp \\\n",
    "    SENTENCE_TRANSFORMERS_HOME=/tmp\n",
    "```\n",
    "\n",
    "* **What it does**: Same as earlier ‚Äî sets cache and tokenizer environment variables.\n",
    "* **Why**: Redundant ‚Äî but harmless. You could keep only one block to simplify.\n",
    "\n",
    "---\n",
    "\n",
    "#### 10. `COPY requirements.txt ./`\n",
    "\n",
    "* **What it does**: Copies your dependency list into the image.\n",
    "* **Why**: So it can install them in the next step.\n",
    "\n",
    "---\n",
    "\n",
    "#### 11. `COPY src/ ./src/`\n",
    "\n",
    "* **What it does**: Copies your actual app code into the container‚Äôs `/app/src` directory.\n",
    "\n",
    "---\n",
    "\n",
    "#### 12. `RUN pip install --no-cache-dir -r requirements.txt`\n",
    "\n",
    "* **What it does**: Installs all required Python libraries.\n",
    "* **Why**: `--no-cache-dir` saves space inside the image.\n",
    "\n",
    "---\n",
    "\n",
    "#### 13. `EXPOSE 8501`\n",
    "\n",
    "* **What it does**: Tells Docker that this container listens on port 8501.\n",
    "* **Why**: Streamlit's default port is 8501.\n",
    "\n",
    "---\n",
    "\n",
    "#### 14. `HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health || exit 1`\n",
    "\n",
    "* **What it does**: Docker checks this endpoint to ensure your app is healthy.\n",
    "* **Why**: Helps automated systems know if the app is running properly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 15. `ENTRYPOINT [\"streamlit\", \"run\", \"src/streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]`\n",
    "\n",
    "* **What it does**: Runs your Streamlit app inside the container.\n",
    "* **Why**:\n",
    "\n",
    "  * `--server.port=8501`: Explicitly binds it to 8501.\n",
    "  * `--server.address=0.0.0.0`: Allows access from any IP (needed in deployment).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Let me know if you want this explained again as a blog-style story like your Masala Dosa Caf√© analogy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb8043e",
   "metadata": {},
   "source": [
    "Here's a **line-by-line explanation** of your `streamlit_app.py` code so you fully understand what each part does and why it was used:\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ **Imports**\n",
    "\n",
    "```python\n",
    "import os\n",
    "import streamlit as st\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "```\n",
    "\n",
    "* `os`: Used for setting environment variables.\n",
    "* `streamlit as st`: The core UI framework for building the app.\n",
    "* `PdfReader`: To read and extract text from PDF files.\n",
    "* `SentenceTransformer`: To generate vector embeddings of text.\n",
    "* `faiss`: For fast similarity search on embeddings.\n",
    "* `numpy`: Used for numerical operations, mainly to convert embeddings to float32.\n",
    "* `OpenAI`: To call the OpenRouter-compatible API (like Mistral).\n",
    "* `requests`: (commented out) was used to test internet or model endpoints.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Environment Setup for Hugging Face on HF Spaces**\n",
    "\n",
    "```python\n",
    "import tempfile, platform\n",
    "if platform.system() == \"Windows\":\n",
    "    base_cache = tempfile.gettempdir()\n",
    "else:\n",
    "    base_cache = \"/tmp\"\n",
    "```\n",
    "\n",
    "* Detects OS and selects a writable temp directory.\n",
    "* HF Spaces has strict permissions; this avoids cache write errors.\n",
    "\n",
    "```python\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_HOME\"] = base_cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = base_cache\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = base_cache\n",
    "```\n",
    "\n",
    "* These are **Hugging Face cache locations**.\n",
    "* Prevents errors like `meta tensor` or `permission denied`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîê **OpenRouter API Setup**\n",
    "\n",
    "```python\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "```\n",
    "\n",
    "* Reads your API key from `.env` (locally) or `HF secrets` (on Hugging Face).\n",
    "\n",
    "```python\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key,\n",
    "    default_headers={...}\n",
    ")\n",
    "```\n",
    "\n",
    "* Initializes the OpenAI client to call Mistral model via OpenRouter API.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Load Embedding Model**\n",
    "\n",
    "```python\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "```\n",
    "\n",
    "* Loads a lightweight transformer model to create sentence embeddings for FAISS.\n",
    "\n",
    "---\n",
    "\n",
    "### üé® **Streamlit Title UI**\n",
    "\n",
    "```python\n",
    "st.markdown(\"\"\"\n",
    "<h1 style=\"...\">üìÑ Chat with Your PDF</h1>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "```\n",
    "\n",
    "* Custom styled title using HTML + gradient text color.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ **Upload and Extract PDF**\n",
    "\n",
    "```python\n",
    "uploaded = st.file_uploader(\"Upload a PDF\", type=\"pdf\")\n",
    "if not uploaded:\n",
    "    st.stop()\n",
    "```\n",
    "\n",
    "* Stops the app if no PDF is uploaded.\n",
    "\n",
    "```python\n",
    "reader = PdfReader(uploaded)\n",
    "raw_text = \"\\n\".join(p.extract_text() or \"\" for p in reader.pages)\n",
    "if not raw_text.strip():\n",
    "    st.warning(\"‚ùó No extractable text found in this PDF.\")\n",
    "    st.stop()\n",
    "```\n",
    "\n",
    "* Reads and extracts all text from each page.\n",
    "* Stops if the PDF is blank or image-scanned without text.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÇÔ∏è **Chunking PDF Text**\n",
    "\n",
    "```python\n",
    "def chunk_text(txt, size=300):\n",
    "    words = txt.split()\n",
    "    return [\" \".join(words[i:i+size]) for i in range(0, len(words), size)]\n",
    "```\n",
    "\n",
    "* Splits large text into smaller chunks for embedding.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Build Embedding Index (FAISS)**\n",
    "\n",
    "```python\n",
    "chunks = chunk_text(raw_text)\n",
    "embeddings = model.encode(chunks).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "```\n",
    "\n",
    "* Converts chunks into vectors and adds them to FAISS index for fast search.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Chat History Initialization**\n",
    "\n",
    "```python\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "```\n",
    "\n",
    "* Preserves conversation across interactions.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Semantic Search + Prompt Creation**\n",
    "\n",
    "```python\n",
    "def semantic_retrieve(query, k=3):\n",
    "    q_vec = model.encode([query]).astype(\"float32\")\n",
    "    _, I = index.search(q_vec, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "```\n",
    "\n",
    "* Finds top `k` similar chunks using FAISS.\n",
    "\n",
    "```python\n",
    "def generate_answer(question):\n",
    "    context = \"\\n\".join(semantic_retrieve(question))\n",
    "    prompt = (\n",
    "        \"Use the context below to answer the question:\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\nAnswer:\"\n",
    "    )\n",
    "```\n",
    "\n",
    "* Builds context-aware prompt using RAG technique.\n",
    "\n",
    "```python\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",\n",
    "        messages=st.session_state.chat_history,\n",
    "        temperature=0.3\n",
    "    )\n",
    "```\n",
    "\n",
    "* Sends prompt to OpenRouter‚Äôs Mistral model.\n",
    "\n",
    "```python\n",
    "    answer = response.choices[0].message.content\n",
    "    st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    return answer\n",
    "```\n",
    "\n",
    "* Stores response in session and returns it.\n",
    "\n",
    "---\n",
    "\n",
    "### üó£Ô∏è **Frontend Chat Interaction**\n",
    "\n",
    "```python\n",
    "with st.form(key=\"chat_form\", clear_on_submit=True):\n",
    "    user_input = st.text_input(\"Ask a question about the PDF\", disabled=not parsed)\n",
    "    submit = st.form_submit_button(\"üì§ Send\", use_container_width=True)\n",
    "```\n",
    "\n",
    "* A form for user input; disables it if PDF parsing failed.\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ **Answer Display with Styling**\n",
    "\n",
    "```python\n",
    "with st.spinner(\"Thinking...\"):\n",
    "    answer = generate_answer(user_input)\n",
    "```\n",
    "\n",
    "* Spinner while waiting for response.\n",
    "\n",
    "```python\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".user-msg { ... }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "```\n",
    "\n",
    "* Applies gradient background for user message.\n",
    "\n",
    "```python\n",
    "st.markdown(f'User: <div class=\"user-msg\">{user_input}</div>', unsafe_allow_html=True)\n",
    "st.markdown(f'Bot: <div class=\"bot-msg\">{answer}</div>', unsafe_allow_html=True)\n",
    "```\n",
    "\n",
    "* Prints user and bot messages as chat bubbles.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want this **documented as a README** or turned into a **PDF report**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab532db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .streamlit/config.toml over docker ENTRY_POINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac723871",
   "metadata": {},
   "source": [
    "Great question! Here‚Äôs **why using `.streamlit/config.toml` is preferred** over setting options in the Docker `ENTRYPOINT` command or via `streamlit run` flags:\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. **Cleaner & More Scalable Configuration**\n",
    "\n",
    "* **`.streamlit/config.toml`** is the **official way** Streamlit expects configuration.\n",
    "* Easy to update features like:\n",
    "\n",
    "  ```toml\n",
    "  [server]\n",
    "  headless = true\n",
    "  port = 8501\n",
    "  enableCORS = false\n",
    "  ```\n",
    "* No need to **rewrite the Dockerfile** every time you want to change a config option.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **Avoids Long ENTRYPOINT Commands**\n",
    "\n",
    "If you hardcode everything in:\n",
    "\n",
    "```dockerfile\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"src/app.py\", \"--server.port=8501\", \"--server.headless=true\", ...]\n",
    "```\n",
    "\n",
    "‚Ä¶it becomes messy, error-prone, and hard to debug.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. **HF Spaces & Permissions**\n",
    "\n",
    "* On **Hugging Face Spaces**, setting configs in `/tmp/.streamlit/config.toml` ensures:\n",
    "\n",
    "  * No permission errors (default `~/.streamlit/` is not writable).\n",
    "  * Streamlit detects it automatically without needing command-line flags.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 4. **Easy Local vs Remote Separation**\n",
    "\n",
    "You can:\n",
    "\n",
    "* Keep `config.toml` for cloud deployment (e.g., set CORS, headless)\n",
    "* Use local flags during dev (`streamlit run app.py --theme.darkMode=true`)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 5. **Future-proofing**\n",
    "\n",
    "* Streamlit may change or deprecate flags, but `config.toml` is stable and versioned.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ TL;DR:\n",
    "\n",
    "**Use `.streamlit/config.toml`** to cleanly separate UI/server configs, avoid Docker clutter, and ensure smooth deployment in restricted environments like HF Spaces.\n",
    "\n",
    "Would you like a sample config.toml for your case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f90214",
   "metadata": {},
   "source": [
    "Excellent! Let‚Äôs answer this in **three parts** to clarify everything:\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. **Why `config.toml` works in Android (mobile browser) but not directly in Docker without config**\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "* When you **run Streamlit locally**, it auto-loads `.streamlit/config.toml` because it knows where your home directory is and can write/read configs.\n",
    "* In **Docker containers on Hugging Face**, your default home directory (`/root`) is often **read-only or restricted**.\n",
    "\n",
    "So:\n",
    "\n",
    "| Environment               | `.streamlit/config.toml` Works?                                                                              | Why?                                                   |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------ |\n",
    "| **Local (macOS/mobile)**  | ‚úÖ Yes                                                                                                        | Can read/write config + no sandbox                     |\n",
    "| **Docker (Hugging Face)** | ‚ùå Not by default unless you **copy to `/tmp/.streamlit`** and set `ENV STREAMLIT_CONFIG_DIR=/tmp/.streamlit` | Because `/root/.streamlit` isn‚Äôt writable in container |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **Why Docker is used in Hugging Face Spaces**\n",
    "\n",
    "Hugging Face Spaces (non-Gradio apps like Streamlit) **runs your code inside a Docker container**. This lets it:\n",
    "\n",
    "* Isolate your app\n",
    "* Install all your dependencies from `requirements.txt`\n",
    "* Recreate your environment consistently\n",
    "\n",
    "#### ‚öôÔ∏è What Docker does for you:\n",
    "\n",
    "1. Base image: Starts with `python:3.9-slim`\n",
    "2. Copies your code and config\n",
    "3. Installs system tools + Python packages\n",
    "4. Exposes port 8501 (for Streamlit)\n",
    "5. Runs your Streamlit app with `ENTRYPOINT`\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ 3. **Flow after you push to Hugging Face Spaces**\n",
    "\n",
    "```text\n",
    "1. You push the repo with Dockerfile & app\n",
    "       ‚Üì\n",
    "2. Hugging Face builds the Docker image:\n",
    "   - Installs Python\n",
    "   - Installs packages from `requirements.txt`\n",
    "   - Copies files, sets ENV variables\n",
    "   - Prepares the entrypoint\n",
    "       ‚Üì\n",
    "3. Container is launched in the cloud (sandboxed)\n",
    "   - Cannot access normal file system (~/.streamlit etc.)\n",
    "   - Must use `/tmp` or `ENV`-based paths\n",
    "       ‚Üì\n",
    "4. Hugging Face routes your app to a public URL\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîë So What Was the Real Fix?\n",
    "\n",
    "‚úÖ You used:\n",
    "\n",
    "```dockerfile\n",
    "COPY .streamlit/ /tmp/.streamlit/\n",
    "ENV STREAMLIT_CONFIG_DIR=/tmp/.streamlit\n",
    "```\n",
    "\n",
    "‚úÖ That told Streamlit in the container:\n",
    "\n",
    "> ‚ÄúLook inside `/tmp/.streamlit/` for config, not in default (\\~/.streamlit)‚Äù\n",
    "\n",
    "**And that worked on mobile because the deployed container respected it ‚Äî mobile just accessed the output via browser.**\n",
    "\n",
    "---\n",
    "\n",
    "Want a diagram or flowchart for this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6890b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d89f3c",
   "metadata": {},
   "source": [
    "Great question. Let‚Äôs walk through it step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùó What is a CORS Error?\n",
    "\n",
    "**CORS = Cross-Origin Resource Sharing**\n",
    "\n",
    "CORS errors happen when:\n",
    "\n",
    "* You load a **web frontend** (like Streamlit in a browser)\n",
    "* That frontend **makes a request to a different domain/server**\n",
    "* The server does **not allow** your frontend‚Äôs domain in its **CORS headers**\n",
    "\n",
    "Browser blocks the request to protect users (security policy).\n",
    "\n",
    "---\n",
    "\n",
    "## üìç When Does This Happen in Streamlit?\n",
    "\n",
    "If you **deploy a Streamlit app on Hugging Face**, it‚Äôs served inside an iframe on:\n",
    "\n",
    "```\n",
    "https://huggingface.co/spaces/<username>/<space-name>\n",
    "```\n",
    "\n",
    "BUT the actual app runs inside:\n",
    "\n",
    "```\n",
    "https://<random-subdomain>.hf.space\n",
    "```\n",
    "\n",
    "Your app is now **served from a different origin**, and Streamlit **blocks iframe embedding** by default unless you allow it.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ How We Handled CORS in `config.toml`\n",
    "\n",
    "Inside `.streamlit/config.toml`, we added:\n",
    "\n",
    "```toml\n",
    "[server]\n",
    "enableCORS = false\n",
    "enableXsrfProtection = false\n",
    "headless = true\n",
    "```\n",
    "\n",
    "### üîç What Each Line Does:\n",
    "\n",
    "| Setting                        | Purpose                                                        |\n",
    "| ------------------------------ | -------------------------------------------------------------- |\n",
    "| `enableCORS = false`           | Disables CORS policy ‚Äî so Hugging Face iframe can load the app |\n",
    "| `enableXsrfProtection = false` | Disables anti-XSRF token check (needed inside iframe)          |\n",
    "| `headless = true`              | Ensures Streamlit runs without GUI (important for deployment)  |\n",
    "\n",
    "---\n",
    "\n",
    "## üåê Full Flow When User Opens Your App:\n",
    "\n",
    "```text\n",
    "1. User visits: https://huggingface.co/spaces/rakshath6/pdf-chat-rag\n",
    "       ‚Üì\n",
    "2. Hugging Face shows an iframe: <iframe src=\"https://xxxxx.hf.space\">\n",
    "       ‚Üì\n",
    "3. Browser loads the Streamlit app inside iframe\n",
    "       ‚Üì\n",
    "4. If CORS is not disabled ‚Üí Browser blocks it ‚ùå\n",
    "       ‚Üì\n",
    "5. But you disabled CORS in config.toml ‚Üí Streamlit allows iframe ‚úÖ\n",
    "       ‚Üì\n",
    "6. App loads perfectly, user sees the UI\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* **CORS error**: Browser blocked Hugging Face iframe from loading your app.\n",
    "* **Fix**: Added `enableCORS = false` in `config.toml`\n",
    "* **Result**: Streamlit allowed the Hugging Face iframe to embed your app successfully.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a flowchart or code-based example of this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198a69d",
   "metadata": {},
   "source": [
    "Great question. Let‚Äôs walk through it step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùó What is a CORS Error?\n",
    "\n",
    "**CORS = Cross-Origin Resource Sharing**\n",
    "\n",
    "CORS errors happen when:\n",
    "\n",
    "* You load a **web frontend** (like Streamlit in a browser)\n",
    "* That frontend **makes a request to a different domain/server**\n",
    "* The server does **not allow** your frontend‚Äôs domain in its **CORS headers**\n",
    "\n",
    "Browser blocks the request to protect users (security policy).\n",
    "\n",
    "---\n",
    "\n",
    "## üìç When Does This Happen in Streamlit?\n",
    "\n",
    "If you **deploy a Streamlit app on Hugging Face**, it‚Äôs served inside an iframe on:\n",
    "\n",
    "```\n",
    "https://huggingface.co/spaces/<username>/<space-name>\n",
    "```\n",
    "\n",
    "BUT the actual app runs inside:\n",
    "\n",
    "```\n",
    "https://<random-subdomain>.hf.space\n",
    "```\n",
    "\n",
    "Your app is now **served from a different origin**, and Streamlit **blocks iframe embedding** by default unless you allow it.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ How We Handled CORS in `config.toml`\n",
    "\n",
    "Inside `.streamlit/config.toml`, we added:\n",
    "\n",
    "```toml\n",
    "[server]\n",
    "enableCORS = false\n",
    "enableXsrfProtection = false\n",
    "headless = true\n",
    "```\n",
    "\n",
    "### üîç What Each Line Does:\n",
    "\n",
    "| Setting                        | Purpose                                                        |\n",
    "| ------------------------------ | -------------------------------------------------------------- |\n",
    "| `enableCORS = false`           | Disables CORS policy ‚Äî so Hugging Face iframe can load the app |\n",
    "| `enableXsrfProtection = false` | Disables anti-XSRF token check (needed inside iframe)          |\n",
    "| `headless = true`              | Ensures Streamlit runs without GUI (important for deployment)  |\n",
    "\n",
    "---\n",
    "\n",
    "## üåê Full Flow When User Opens Your App:\n",
    "\n",
    "```text\n",
    "1. User visits: https://huggingface.co/spaces/rakshath6/pdf-chat-rag\n",
    "       ‚Üì\n",
    "2. Hugging Face shows an iframe: <iframe src=\"https://xxxxx.hf.space\">\n",
    "       ‚Üì\n",
    "3. Browser loads the Streamlit app inside iframe\n",
    "       ‚Üì\n",
    "4. If CORS is not disabled ‚Üí Browser blocks it ‚ùå\n",
    "       ‚Üì\n",
    "5. But you disabled CORS in config.toml ‚Üí Streamlit allows iframe ‚úÖ\n",
    "       ‚Üì\n",
    "6. App loads perfectly, user sees the UI\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* **CORS error**: Browser blocked Hugging Face iframe from loading your app.\n",
    "* **Fix**: Added `enableCORS = false` in `config.toml`\n",
    "* **Result**: Streamlit allowed the Hugging Face iframe to embed your app successfully.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a flowchart or code-based example of this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd7b7b",
   "metadata": {},
   "source": [
    "**CORS is saying to the browser:**\n",
    "\n",
    "> ‚ùå ‚ÄúHey! You‚Äôre trying to load content from another site inside a frame ‚Äî but that site didn‚Äôt say it‚Äôs okay. Block it!‚Äù\n",
    "\n",
    "That‚Äôs why your app needed `enableCORS = false` ‚Äî to say:\n",
    "\n",
    "> ‚úÖ ‚ÄúYes, I allow being loaded inside Hugging Face‚Äôs iframe.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004eb15",
   "metadata": {},
   "source": [
    "üöÄ Phase 4: Full-Stack AI Product\n",
    "1. Add multi-document support\n",
    "Upload multiple PDFs\n",
    "\n",
    "Store them in FAISS with document titles\n",
    "\n",
    "Retrieve from relevant document\n",
    "\n",
    "2. Store chat history (optional)\n",
    "Use SQLite / Supabase / Firebase to store past chats per user.\n",
    "\n",
    "3. User authentication (auth)\n",
    "Use streamlit-authenticator or migrate to Next.js + FastAPI for login/signup\n",
    "\n",
    "4. Frontend Upgrade (React)\n",
    "Shift from Streamlit to React + Tailwind\n",
    "\n",
    "Backend: FastAPI or Flask serving FAISS + OpenRouter\n",
    "\n",
    "5. Analytics / Feedback\n",
    "Track questions, feedback\n",
    "\n",
    "Show sources from PDF in output (like ‚ÄúAnswer sourced from Page 3‚Äù)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5ffbc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6276a954",
      "metadata": {
        "id": "6276a954"
      },
      "source": [
        "**LLM Fine-tuning path ‚Äì Part 1: LoRA + QLoRA**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Step 1: What is LoRA & Why Use It?\n",
        "\n",
        "üß† **LoRA (Low-Rank Adaptation)** is a lightweight way to fine-tune big models (like LLaMA, Falcon, Mistral) **without changing all model weights**.\n",
        "\n",
        "**Story Style:**\n",
        "Imagine you‚Äôve trained to be a cricketer üèè. Instead of retraining your entire body, LoRA is like just changing your *batting stance*. It‚Äôs faster, lighter, and saves energy.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Step 2: Key Concepts\n",
        "\n",
        "* **Full Fine-tuning**: Changes *all* model weights ‚Üí slow, memory-heavy\n",
        "* **LoRA Fine-tuning**: Freezes base model, adds *tiny trainable adapters*\n",
        "* **QLoRA**: Combines LoRA + 4-bit quantization = super low RAM usage (\\~<8GB!)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Step 3: Tools We'll Use\n",
        "\n",
        "* ü§ó Hugging Face `transformers`\n",
        "* ü§ó `peft` for LoRA/QLoRA\n",
        "* `bitsandbytes` for quantization\n",
        "* Google Colab or local GPU (min 12GB VRAM recommended for full-size models)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "kOZIFrRZm3ls",
      "metadata": {
        "id": "kOZIFrRZm3ls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7Wm_Mlkmmmqo",
      "metadata": {
        "id": "7Wm_Mlkmmmqo"
      },
      "source": [
        "Run Your First LoRA Fine-tuning Example** using Hugging Face PEFT.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Goal\n",
        "\n",
        "Fine-tune a small LLM (like `google/flan-t5-small`) on a **custom instruction dataset** using **LoRA + PEFT**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ What You'll Learn\n",
        "\n",
        "* Use PEFT + LoRA to fine-tune a model\n",
        "* Load custom dataset in Hugging Face format\n",
        "* Track training progress with `transformers.Trainer`\n",
        "* Save and use your new fine-tuned model\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Next Step\n",
        "\n",
        "I‚Äôll walk you through:\n",
        "\n",
        "1. **Installing all dependencies**\n",
        "2. **Loading a dataset**\n",
        "3. **Loading base model**\n",
        "4. **Applying LoRA with PEFT**\n",
        "5. **Training using Trainer API**\n",
        "6. **Saving and loading the model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c022677b",
      "metadata": {},
      "source": [
        "‚úÖ Goal\n",
        "Fine-tune a small LLM (like google/flan-t5-small) on a custom instruction dataset using LoRA + PEFT.\n",
        "\n",
        "‚úÖ Step 4.2: Load Model + Dataset for LoRA Fine-tuning\n",
        "Let‚Äôs load a base LLM and a small dataset to fine-tune.\n",
        "\n",
        "üß† We'll Use:\n",
        "Base Model: tiiuae/falcon-7b-instruct or tiiuae/falcon-rw-1b (for Colab/low-RAM)\n",
        "\n",
        "Dataset: ybelkada/english_quotes ‚Äì small quote dataset for fast training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x0DGiWUZmxoy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0DGiWUZmxoy",
        "outputId": "725150de-00be-4d51-fec8-540e16b8f447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m92.5/207.5 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IuQ_rJ3rm6Ts",
      "metadata": {
        "id": "IuQ_rJ3rm6Ts"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cd36adda",
      "metadata": {},
      "source": [
        "### üß† What You Just Installed\n",
        "\n",
        "* `transformers` ‚Äì Models + Trainer\n",
        "* `datasets` ‚Äì Load datasets easily\n",
        "* `peft` ‚Äì Parameter Efficient Fine-Tuning (LoRA, QLoRA)\n",
        "* `accelerate` ‚Äì For training optimizations\n",
        "* `bitsandbytes` ‚Äì 8-bit quantization backend"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VqDDBlM9MYTT",
      "metadata": {
        "id": "VqDDBlM9MYTT"
      },
      "source": [
        "‚úÖ Why this?\n",
        "\n",
        "- peft: For LoRA / QLoRA fine-tuning\n",
        "- transformers: Pretrained models (we‚Äôll use a small one)\n",
        "- datasets: Load sample datasets\n",
        "- bitsandbytes: For 8-bit loading (saves memory)\n",
        "- accelerate: Optimized training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o0itV2xeL846",
      "metadata": {
        "id": "o0itV2xeL846"
      },
      "outputs": [],
      "source": [
        "# Load a Small Base Model in 8-bit\n",
        "# We'll use tiiuae/falcon-rw-1b, a 1B parameter model ‚Äì small and LoRA-friendly.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"tiiuae/falcon-rw-1b\"\n",
        "\n",
        "# Load model in 8-bit mode (saves VRAM/RAM)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WfFNUpbwOML0",
      "metadata": {
        "id": "WfFNUpbwOML0"
      },
      "source": [
        "### üí° What does **8-bit** mean?\n",
        "\n",
        "By default, model weights are stored in **32-bit floating point (FP32)** ‚Äî this uses a lot of RAM/VRAM.\n",
        "\n",
        "‚û°Ô∏è **8-bit loading** compresses each weight from 32 bits ‚Üí 8 bits using [bitsandbytes](https://github.com/TimDettmers/bitsandbytes), reducing memory usage **by \\~75%**.\n",
        "\n",
        "#### ‚úÖ Benefits of 8-bit:\n",
        "\n",
        "* Load **large models** in **low memory** (like Colab‚Äôs free tier).\n",
        "* Slight speed boost in inference.\n",
        "* Enables fine-tuning on **consumer hardware**.\n",
        "\n",
        "#### ‚ùó Drawback:\n",
        "\n",
        "* A tiny loss in precision (but usually unnoticeable for fine-tuning tasks).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Why use `device_map=\"auto\"`?\n",
        "\n",
        "This tells Hugging Face to **automatically decide** where to put the model:\n",
        "\n",
        "* If you have GPU: puts model on GPU.\n",
        "* If no GPU: uses CPU.\n",
        "* If limited memory: **splits layers** between CPU and GPU.\n",
        "\n",
        "#### ‚úÖ Benefits:\n",
        "\n",
        "* You **don‚Äôt need to worry** about manually placing tensors.\n",
        "* Works well across Colab / system / any setup.\n",
        "\n",
        "---\n",
        "\n",
        "next step is preparing dataset for fine-tuning (`Alpaca-style`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cKeH1aXvOM4L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKeH1aXvOM4L",
        "outputId": "8ea0cd8f-ecd6-46fb-8d80-ca55988a11b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset ready\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Step 4: Prepare Dataset for Fine-Tuning (Alpaca-style)\n",
        "# We‚Äôll use Alpaca-style format ‚Äî perfect for instruction-tuned models.\n",
        "# üóÇÔ∏è Format: JSON (List of Dicts)\n",
        "\n",
        "import json\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        \"instruction\": \"What is the capital of India?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"New Delhi\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Summarize the text\",\n",
        "        \"input\": \"Artificial intelligence is the simulation of human intelligence processes by machines.\",\n",
        "        \"output\": \"AI simulates human intelligence using machines.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Convert to binary\",\n",
        "        \"input\": \"7\",\n",
        "        \"output\": \"111\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Write a thank you note\",\n",
        "        \"input\": \"To my teacher for helping me learn Python.\",\n",
        "        \"output\": \"Thank you for guiding me through Python and making learning fun!\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Translate to Kannada\",\n",
        "        \"input\": \"Hello, how are you?\",\n",
        "        \"output\": \"‡≤π‡≤≤‡≥ã, ‡≤®‡≥Ä‡≤µ‡≥Å ‡≤π‡≥á‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Ä‡≤∞‡≤ø?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "with open(\"sample_alpaca.json\", \"w\") as f:\n",
        "    json.dump(data, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Dataset ready\")\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "LvV9chAlQeqB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "LvV9chAlQeqB",
        "outputId": "d8828f01-3aa3-4975-c54e-3fe29141d041"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2-2843033788.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2-2843033788.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install datasets\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#Install HuggingFace datasets\n",
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "glVWQelhSEGN",
      "metadata": {
        "id": "glVWQelhSEGN"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "r3ygFqUzPEjK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3ygFqUzPEjK",
        "outputId": "2d65b58d-d85f-4538-9f4e-07f9c8664cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'instruction': 'What is the capital of India?', 'input': '', 'output': 'New Delhi'}\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Step 5: Load Dataset + Tokenize (Alpaca-style)\n",
        "# We now load your JSON file and tokenize it to feed into the LLM.\n",
        "\n",
        "# üì• Step 5.2 ‚Äì Load JSON Dataset\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# dataset = load_dataset(\"json\", data_files=\"sample_alpaca.json\")\n",
        "# dataset = dataset[\"train\"]  # Just get the 'train' split\n",
        "\n",
        "# Load JSON manually\n",
        "with open(\"sample_alpaca.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert list of dicts to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset[0]\n",
        "\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SHjOLZixTb8f",
      "metadata": {
        "id": "SHjOLZixTb8f"
      },
      "source": [
        "# Step 5.3 ‚Äì Format for Instruction Tuning\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "{output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "knjcHF4iPcMN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c10812a919974c90bcf577c7f6c1fb34",
            "a36a2ac89226489e8bca6865d0dae14d",
            "ed41ca9e45e84ac997cd30652a3a2bdf",
            "8b819112bdb7474490a8d8fc7ad1756b",
            "96c42403ee5048578f5ad123538362f0",
            "fda97e1770e94ac78bd7a3a06e910090",
            "b54f587ce5dc4b3298c7e42d1eea711f",
            "d3ea4589e0f0401a9f5fbcd365ab97d4",
            "2256b603cd234f2ea1162fcac670e1e7",
            "037a5d4b998a4236b207ea0b0dc56712",
            "b7a18bff47564bb286567c674ba3429c"
          ]
        },
        "id": "knjcHF4iPcMN",
        "outputId": "6135486f-eebb-43be-c139-d35114aeb84b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c10812a919974c90bcf577c7f6c1fb34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def format_prompt(example):\n",
        "    if example[\"input\"]:\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Input:\n",
        "{example['input']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "\n",
        "dataset = dataset.map(lambda ex: {\"text\": format_prompt(ex)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9sPRTCaZTTqW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "06c522a629924d2ab28cac95093f084a",
            "d2fe5b15027c412f9c874fef838a2bf8",
            "f71f3644c615407cabb67993b93d99d9",
            "fc19acd2bfdf4f36aff0f7c7c6a5ba7d",
            "df22c95132404309af5bc91a2a4c5f35",
            "fc7ed83cff4f4aaba55f4688e21a27d6",
            "b5e9a2b001af452aabaad9c55a423f6c",
            "28db79e180c54dccb35605429b5d2ee3",
            "7dc3d986d00841ada6e9cb437f2dd482",
            "8c588711ce494c17a2e4bcdd270c08fe",
            "9c44b73b4f504b6594444d1fc0609d98"
          ]
        },
        "id": "9sPRTCaZTTqW",
        "outputId": "c9baad16-f11f-49eb-f318-19284832fb72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06c522a629924d2ab28cac95093f084a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ü§ñ Step 5.4 ‚Äì Tokenize the Text\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "tokenized = dataset.map(lambda x: tokenizer(x[\"text\"]), batched=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ivMSBSyZURwN",
      "metadata": {
        "id": "ivMSBSyZURwN"
      },
      "source": [
        "Great ‚Äî this `format_prompt` function is very important. It prepares your **training data in the format that the LLM expects**.\n",
        "\n",
        "Let‚Äôs break it down line by line:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Purpose:\n",
        "\n",
        "To turn each example (a dictionary like below):\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"instruction\": \"Translate English to French.\",\n",
        "  \"input\": \"I love AI.\",\n",
        "  \"output\": \"J'aime l'IA.\"\n",
        "}\n",
        "```\n",
        "\n",
        "Into a **single training string**, like this:\n",
        "\n",
        "```\n",
        "### Instruction:\n",
        "Translate English to French.\n",
        "\n",
        "### Input:\n",
        "I love AI.\n",
        "\n",
        "### Response:\n",
        "J'aime l'IA.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Code Breakdown:\n",
        "\n",
        "```python\n",
        "def format_prompt(example):\n",
        "```\n",
        "\n",
        "Defines a function that takes **one training sample (a dict)**.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "    if example[\"input\"]:\n",
        "```\n",
        "\n",
        "If there **is an input field** (some datasets don‚Äôt have it), then:\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Input:\n",
        "{example['input']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "```\n",
        "\n",
        "‚Üí Format with:\n",
        "\n",
        "* **Instruction**\n",
        "* **Input**\n",
        "* **Response**\n",
        "\n",
        "Used in many Alpaca-style fine-tuning datasets.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "    else:\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "```\n",
        "\n",
        "‚Üí If no `\"input\"` exists, skip it.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Then apply it to dataset:\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(lambda ex: {\"text\": format_prompt(ex)})\n",
        "```\n",
        "\n",
        "This:\n",
        "\n",
        "* Applies `format_prompt()` to every row (`ex`)\n",
        "* Adds a new `\"text\"` field to each example\n",
        "* Final dataset looks like:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"text\": \"### Instruction: ... \\n\\n### Input: ... \\n\\n### Response: ...\"\n",
        "}\n",
        "```\n",
        "\n",
        "This is the **final prompt format** that will be fed to the LLM during training.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know when you're ready to move to the **Tokenizer + LoRA config step**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "LYUAXq2Zwmkb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYUAXq2Zwmkb",
        "outputId": "7fc1b02f-6ed2-4a40-a907-d5d4e6e5e3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: keras 3.8.0\n",
            "Uninstalling keras-3.8.0:\n",
            "  Successfully uninstalled keras-3.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow tensorflow-gpu keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "sko2Be89xXeG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sko2Be89xXeG",
        "outputId": "3996ff92-811b-4ac7-b613-9b3d6a733994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "199747c0bf87440abedf4a55b4fe372e",
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -U bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "wMETkpEwTvay",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "4a688b8454bc479289f03228d962ea71",
            "815a2447343c4d168bf98659e3c8ba16",
            "de17459f594540b0be050ad4304ee422",
            "3aa999179ef34883ade4dc4f52d31170",
            "318977280bf8482eb693e24c1c3540ff",
            "ac4f107127d7473ab7d6315102f6ad16",
            "49fffde822ee4c04bc3a2f62679f97a2",
            "5f6fddd65e95466ea5e92e483e7c4f26",
            "c2d5c55557154b6aab681d274c9e60c5",
            "e3f290ea229c460da2101c0031fef80a",
            "dc6086264901457482a00d56793eb551",
            "9430c83e78e34ef38bb5df9d188a2ea0",
            "538f7f2354fe49408d444d61edd53465",
            "6d7970be944b40be81f2218b57fe1486",
            "b5ad66532b9c43b69462d28da53a1ab9",
            "294f4c44d8ff4e8296e51c1ff252df48",
            "91311cf847c041bab4f771ec64efb700",
            "e81de64a92ef4813b68e238b4bc1cfb2",
            "c422a0ff083e45a6bd7162f6a637552f",
            "5eb22dfd40744855a47204f136204953",
            "579054c6790c4238be2bc2ff4b569297",
            "9ddc08baefdd4e8f96bb98354ac20bfb"
          ]
        },
        "id": "wMETkpEwTvay",
        "outputId": "14ea8ed2-c342-4ab6-8da9-845cab5d4cb4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a688b8454bc479289f03228d962ea71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9430c83e78e34ef38bb5df9d188a2ea0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Step 6: LoRA Configuration + Trainer Setup.\n",
        "\n",
        "# üß† Goal:\n",
        "# Load a Tokenizer\n",
        "# Configure LoRA with PEFT\n",
        "# Use Trainer to fine-tune the model using your formatted prompts\n",
        "\n",
        "# ‚úÖ 1. Load Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\") #This loads the tokenizer and ensures sequences can be padded properly.\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Fix padding token issue\n",
        "\n",
        "# ‚úÖ 2. Tokenize the Dataset\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True) # Now the dataset is tokenized and ready to feed into the model.\n",
        "\n",
        "# ‚úÖ Add labels for loss computation\n",
        "tokenized_dataset = tokenized_dataset.map(\n",
        "    lambda x: {\"labels\": x[\"input_ids\"]},\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# ‚úÖ 3. LoRA Configuration (PEFT)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # works for most models\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM # This sets up a lightweight fine-tuning configuration (LoRA) to save memory and speed up training.\n",
        ")\n",
        "\n",
        "# . Loaded Base Model (8-bit for Low RAM) - Loads model in 8-bit (saves VRAM/RAM).\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "# ‚úÖ 5. Apply LoRA to Base Model\n",
        "from peft import get_peft_model\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters() # Only a small subset of parameters (LoRA adapters) will be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DvmRXiaZklOE",
      "metadata": {
        "id": "DvmRXiaZklOE"
      },
      "source": [
        "Perfect ‚Äî you're doing everything right.\n",
        "\n",
        "Let‚Äôs break down each block of code **step-by-step** with full explanation, especially the `LoRAConfig` section.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 1. Load Tokenizer\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "```\n",
        "\n",
        "### üîç What it does:\n",
        "\n",
        "* Loads the pretrained tokenizer for `TinyLlama`.\n",
        "* Tokenizer breaks your prompt into numerical tokens so the model can understand it.\n",
        "* Some models (like TinyLlama) don‚Äôt have a special **padding token**, so we set:\n",
        "\n",
        "  ```python\n",
        "  pad_token = eos_token\n",
        "  ```\n",
        "\n",
        "  This ensures that when padding is required (to equalize length), it uses the end-of-sequence token.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 2. Tokenize the Dataset\n",
        "\n",
        "```python\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "```\n",
        "\n",
        "### üîç What it does:\n",
        "\n",
        "* Defines a function that:\n",
        "\n",
        "  * Converts text into tokens (`input_ids`)\n",
        "  * Truncates long text beyond `512` tokens\n",
        "  * Pads short text to exactly `512` tokens\n",
        "\n",
        "* Applies it to the full dataset using `.map(...)`\n",
        "\n",
        "Now your dataset has:\n",
        "\n",
        "```python\n",
        "{'input_ids': [...], 'attention_mask': [...]}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 3. LoRA Configuration\n",
        "\n",
        "```python\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "```\n",
        "\n",
        "### üîç Full Breakdown of Parameters:\n",
        "\n",
        "| Param                                 | Meaning                                                                                                                                                                              |\n",
        "| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| `r=8`                                 | Rank of the LoRA adapter ‚Äî the size of the small matrix inserted. Lower = smaller/faster, higher = more capacity.                                                                    |\n",
        "| `lora_alpha=16`                       | Scaling factor. LoRA output is scaled by `alpha / r`. Controls the impact of LoRA.                                                                                                   |\n",
        "| `target_modules=[\"q_proj\", \"v_proj\"]` | These are the attention submodules inside the transformer you apply LoRA to ‚Äî **Q (query)** and **V (value)** in attention layers. This is standard for models like LLaMA, GPT, etc.\t- Which parts of the model to inject LoRA into (e.g., query & value proj)|\n",
        "| `lora_dropout=0.05`                   | Dropout during training inside LoRA adapters. Helps prevent overfitting.                                                                                                             |\n",
        "| `bias=\"none\"`                         | Means LoRA will not touch any bias parameters (only weights).                                                                                                                        |\n",
        "| `task_type=TaskType.CAUSAL_LM`        | This tells PEFT that we‚Äôre fine-tuning a **Causal Language Model** (like GPT-style ‚Äî autoregressive).                                                                                |\n",
        "\n",
        "> ‚úÖ In short: You're injecting tiny trainable weights inside key attention layers (`q_proj`, `v_proj`) and freezing the rest of the model.\n",
        "\n",
        "---\n",
        "## 4. Loaded Base Model (8-bit for Low RAM)\n",
        "```python\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "```\n",
        "- Loads model in 8-bit (saves VRAM/RAM).\n",
        "- device_map=\"auto\" lets ü§ó automatically assign model to GPU or CPU.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## ‚úÖ 5. Apply LoRA to Base Model\n",
        "\n",
        "```python\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "```\n",
        "- Injects LoRA adapters into the model.\n",
        "- print_trainable_parameters() confirms only LoRA adapters are trainable (not the whole model).\n",
        "\n",
        "\n",
        "### üîç What it does:\n",
        "\n",
        "* Takes your base model and **wraps** it with LoRA adapters.\n",
        "* You‚Äôre no longer training the full 1B+ parameters, only \\~few million.\n",
        "* `print_trainable_parameters()` shows how many parameters are being trained ‚Äî should be small (\\~0.1%‚Äì1%).\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ The model you're using here is:\n",
        "\n",
        "```\n",
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
        "```\n",
        "\n",
        "### üîç What is this model?\n",
        "\n",
        "* **Model name:** `TinyLlama-1.1B-Chat-v1.0`\n",
        "* **Size:** \\~1.1 Billion parameters (very small compared to GPT-3/4)\n",
        "* **Type:** Causal Language Model (Chat-optimized)\n",
        "* **Source:** Hugging Face ü§ó model hub\n",
        "* **Use-case:** Lightweight chat-based fine-tuning, great for LoRA experiments\n",
        "\n",
        "This model is:\n",
        "\n",
        "* Fast to train with LoRA\n",
        "* Compatible with 8-bit/4-bit loading (low RAM usage)\n",
        "* Enough for educational projects & small chatbots\n",
        "\n",
        "---\n",
        "\n",
        "You're fine-tuning this model using **LoRA** + **PEFT** + **Trainer**, which is a great combo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XiR2aB0pkljw",
      "metadata": {
        "id": "XiR2aB0pkljw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "HT7AMNixlch6",
      "metadata": {
        "id": "HT7AMNixlch6"
      },
      "outputs": [],
      "source": [
        "# üß™ Step 7: Fine-tune the Model Using transformers.Trainer\n",
        "# We'll use Hugging Face‚Äôs Trainer class to fine-tune the model using your LoRA adapters + tokenized dataset.\n",
        "\n",
        "# ‚úÖ 1. Training Arguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"no\",\n",
        "    # evaluation_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5DUp7E07nMlu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "5DUp7E07nMlu",
        "outputId": "e938d231-d935-437f-c89f-37df0c46702b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=16.384363810221355, metrics={'train_runtime': 3.9134, 'train_samples_per_second': 3.833, 'train_steps_per_second': 1.533, 'total_flos': 47722235166720.0, 'train_loss': 16.384363810221355, 'epoch': 3.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ‚úÖ 2. Initialize Trainer\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "# ‚úÖ 3. Start Training üöÄ\n",
        "trainer.train()  # This will begin fine-tuning using only the LoRA adapters (very lightweight training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "kDpT8Qao0sGB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDpT8Qao0sGB",
        "outputId": "3a5cc83d-47ff-45db-9078-3dc33cd9c729"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Convert to binary\n",
            "\n",
            "### Input:\n",
            "13\n",
            "\n",
            "### Response:\n",
            "1010\n",
            "\n",
            "### Explanation:\n",
            "The input number is 13, and the function converts it to binary using the binary representation of 13. The result is 1010.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Step 8: Inference with Fine-Tuned LoRA Model\n",
        "from transformers import pipeline # You can now prompt the model and see how it responds based on your training.\n",
        "\n",
        "# Create a text generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Try a prompt (same Alpaca format!)\n",
        "# prompt = \"\"\"### Instruction:\n",
        "# Translate to Kannada\n",
        "\n",
        "# ### Input:\n",
        "# Good morning\n",
        "\n",
        "# ### Response:\"\"\"\n",
        "\n",
        "prompt = \"\"\"### Instruction:\n",
        "Convert to binary\n",
        "\n",
        "### Input:\n",
        "13\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "\n",
        "# Generate answer\n",
        "output = pipe(prompt, max_new_tokens=50, do_sample=True)\n",
        "print(output[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daup5dRg0rwY",
      "metadata": {
        "id": "daup5dRg0rwY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "zQe3ZxJ_1fy1",
      "metadata": {
        "id": "zQe3ZxJ_1fy1"
      },
      "source": [
        "Yes, you're absolutely right ‚Äî the model gave the **wrong answer**.\n",
        "\n",
        "### ‚ùå Model Output:\n",
        "\n",
        "```\n",
        "13 ‚Üí 1010 ‚úÖ Wrong\n",
        "```\n",
        "\n",
        "### ‚úÖ Correct Binary:\n",
        "\n",
        "```\n",
        "13 ‚Üí 1101\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why this Happened:\n",
        "\n",
        "* You fine-tuned the model on **only 5 examples**, including **just one \"Convert to binary\"** instruction:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"instruction\": \"Convert to binary\",\n",
        "    \"input\": \"7\",\n",
        "    \"output\": \"111\"\n",
        "  }\n",
        "  ```\n",
        "* That‚Äôs not enough for the model to **generalize the logic** for binary conversion.\n",
        "* It likely just \"memorized\" a pattern like \"Convert ‚Üí ends in 1's\" but has no arithmetic reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Fixes / Next Steps:\n",
        "\n",
        "1. **Add more examples** of binary conversion:\n",
        "\n",
        "   * 5, 10, 15, 20, 30, etc. with correct output.\n",
        "\n",
        "2. Retrain the model with the updated dataset.\n",
        "\n",
        "   Example:\n",
        "\n",
        "   ```json\n",
        "   {\n",
        "     \"instruction\": \"Convert to binary\",\n",
        "     \"input\": \"13\",\n",
        "     \"output\": \"1101\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "3. (Optional) Use a tool-calling agent or calculator tool for actual logic-based tasks instead of fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to generate a better dataset for fine-tuning, or move to the next concept like saving + uploading the model to Hugging Face Hub?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jZ39L0SL3hEV",
      "metadata": {
        "id": "jZ39L0SL3hEV"
      },
      "source": [
        "Here's a full summary of **what you did in fine-tuning**, step-by-step ‚Äî covering purpose, process, flow, and how it‚Äôs useful:\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Goal of Fine-Tuning**\n",
        "\n",
        "Fine-tuning lets you **adapt a general LLM** (like TinyLLaMA) to **perform better on specific tasks or tone**, using custom data.\n",
        "\n",
        "In your case, you fine-tuned `TinyLlama` using **LoRA (Low-Rank Adaptation)** ‚Äî a memory-efficient method that trains only small adapter layers instead of the whole model.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **Why LoRA?**\n",
        "\n",
        "* ‚úÖ Lightweight (just trains a few % of model params)\n",
        "* ‚úÖ Fast to train on Colab or CPU\n",
        "* ‚úÖ Keeps base model frozen (safe, reusable)\n",
        "* ‚úÖ Adds task-specific skill without full retraining\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è **What You Did (Step-by-Step)**\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 1: Prepare Dataset (Instruction Tuning Format)\n",
        "\n",
        "You created a `sample_alpaca.json` file:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"Translate to Kannada\",\n",
        "  \"input\": \"Hello\",\n",
        "  \"output\": \"‡≤π‡≤≤‡≥ã\"\n",
        "}\n",
        "```\n",
        "\n",
        "Purpose: Teach the model to follow instructions ‚Äî e.g., translate, summarize, generate.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 2: Load and Format Dataset\n",
        "\n",
        "You loaded the JSON as a Hugging Face `Dataset` and converted each row into a single **prompt string**:\n",
        "\n",
        "```python\n",
        "### Instruction:\n",
        "Translate to Kannada\n",
        "\n",
        "### Input:\n",
        "Hello\n",
        "\n",
        "### Response:\n",
        "‡≤π‡≤≤‡≥ã\n",
        "```\n",
        "\n",
        "‚û°Ô∏è This format mimics how instruction-tuned models like Alpaca were trained.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 3: Tokenize the Text\n",
        "\n",
        "You used `AutoTokenizer` to tokenize the prompt into input\\_ids the model can understand.\n",
        "\n",
        "```python\n",
        "tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 4: Load Base Model (TinyLlama)\n",
        "\n",
        "```python\n",
        "AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "```\n",
        "\n",
        "* ‚úÖ Loaded in 8-bit (to save memory)\n",
        "* ‚úÖ `device_map=\"auto\"` auto-assigns layers to CPU or GPU\n",
        "* ‚úÖ TinyLlama has \\~1B params ‚Üí perfect for Colab\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 5: Add LoRA Adapter\n",
        "\n",
        "You used PEFT‚Äôs `LoraConfig` to add trainable adapters:\n",
        "\n",
        "```python\n",
        "LoraConfig(\n",
        "    r=8,                   # Rank of adapter matrix\n",
        "    lora_alpha=16,         # Scaling factor\n",
        "    lora_dropout=0.05,     # Regularization\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Only train attention layers\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "```\n",
        "\n",
        "This wraps the model and **adds small trainable layers**, leaving the base untouched.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 6: Train the Model (Trainer API)\n",
        "\n",
        "```python\n",
        "Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "```\n",
        "\n",
        "* Used `Trainer` from Hugging Face Transformers\n",
        "* Trained on 5 examples √ó 3 epochs = ‚úÖ Success!\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Step 7: Results\n",
        "\n",
        "```python\n",
        "trainer.train()\n",
        "```\n",
        "\n",
        "‚û°Ô∏è You got a successful training output like:\n",
        "\n",
        "```\n",
        "TrainOutput(global_step=6, training_loss=16.38)\n",
        "```\n",
        "\n",
        "Not perfect (small dataset), but **LoRA weights learned the instruction pattern**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What You Learned / Achieved\n",
        "\n",
        "| ‚úÖ | You Learned                                              |\n",
        "| - | -------------------------------------------------------- |\n",
        "| ‚úÖ | What LoRA is and why it‚Äôs efficient                      |\n",
        "| ‚úÖ | How to format instruction-tuning datasets                |\n",
        "| ‚úÖ | How to fine-tune TinyLlama on your own data              |\n",
        "| ‚úÖ | How to tokenize, add adapters, and train using `Trainer` |\n",
        "| ‚úÖ | You can now customize LLMs for *your own use cases*      |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ How It‚Äôs Useful\n",
        "\n",
        "* üîß Build your **own assistant** trained on your docs, company data, or style\n",
        "* üí¨ Customize LLM behavior (tone, accuracy, format)\n",
        "* üß† Transfer to bigger datasets later for production\n",
        "* üåç Push to Hugging Face and serve your own model\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you'd like this as a Markdown summary for your GitHub or notes ‚Äî or we can now move to:\n",
        "\n",
        "* ‚úÖ Push to HF\n",
        "* ‚úÖ Use in chatbot\n",
        "* ‚úÖ Train on full Alpaca dataset\n",
        "\n",
        "Which next?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tQ40q5tR1gXA",
      "metadata": {
        "id": "tQ40q5tR1gXA"
      },
      "outputs": [],
      "source": [
        "attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b43SUf04hcK",
      "metadata": {
        "id": "2b43SUf04hcK"
      },
      "source": [
        "Perfect ‚Äî let‚Äôs break this down like a beginner. We‚Äôll cover:\n",
        "\n",
        "1. ‚úÖ **How Fine-tuning (LoRA) relates to Supervised Learning**\n",
        "2. ‚úÖ **What Attention Layers are (in plain English)**\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ1. Is Fine-Tuning the Same as Supervised Learning?\n",
        "\n",
        "Yes! **Fine-tuning = Supervised Learning on an LLM.**\n",
        "\n",
        "| üîç Supervised Learning                        | üß† Fine-Tuning LLM                                        |\n",
        "| --------------------------------------------- | --------------------------------------------------------- |\n",
        "| Trains a model using **input ‚Üí output** pairs | Trains a language model using **prompt ‚Üí response** pairs |\n",
        "| e.g., ‚Äú2 + 2‚Äù ‚Üí 4                             | e.g., ‚ÄúTranslate to Kannada: Hello‚Äù ‚Üí ‚Äú‡≤π‡≤≤‡≥ã‚Äù               |\n",
        "| Learns patterns from **labelled data**        | Also learns from **labelled text examples**               |\n",
        "| You train **the whole model**                 | In LoRA, you train **only small adapters**                |\n",
        "\n",
        "So:\n",
        "\n",
        "* Fine-tuning is a **type of supervised learning**,\n",
        "* But we do it efficiently using **LoRA** to save compute.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ2. What Are Attention Layers in AI? (Simple Explanation)\n",
        "\n",
        "Let‚Äôs break it down like a story:\n",
        "\n",
        "---\n",
        "\n",
        "### üìñ Imagine Reading a Sentence:\n",
        "\n",
        "> \"The **cat** sat on the **mat**, and it looked happy.\"\n",
        "\n",
        "Now, what does **\"it\"** refer to?\n",
        "\n",
        "You **pay attention** to the word \"cat\" earlier in the sentence to understand \"it\" = cat.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ In Transformers‚Ä¶\n",
        "\n",
        "Attention layers **do the same thing** ‚Äî they let the model:\n",
        "\n",
        "* Look at **all words in the sentence** (not just one at a time)\n",
        "* Assign **\"attention scores\"** to decide which words are important\n",
        "* Focus more on some words than others while generating the output\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Example: ‚ÄúTranslate: The cat sat on the mat.‚Äù\n",
        "\n",
        "* Attention layer helps model focus on **\"cat\"**, **\"sat\"**, and **\"mat\"** while translating\n",
        "* Without attention, it might just generate random words\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Summary:\n",
        "\n",
        "| Term               | Meaning                                                    |\n",
        "| ------------------ | ---------------------------------------------------------- |\n",
        "| **Attention**      | Mechanism to focus on relevant words/tokens                |\n",
        "| **Self-Attention** | Words paying attention to other words in the same sentence |\n",
        "| **Used in**        | Transformers (BERT, GPT, etc.)                             |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why It Matters for You\n",
        "\n",
        "In LoRA, you trained only:\n",
        "\n",
        "```python\n",
        "target_modules = [\"q_proj\", \"v_proj\"]\n",
        "```\n",
        "\n",
        "These are **parts of the attention layers** ‚Äî the \"Query\" and \"Value\" projections.\n",
        "\n",
        "So you're fine-tuning **how the model pays attention** to inputs based on your data!\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a story-style visual to understand Attention using a real-life example (like school, WhatsApp chats, or cooking)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Okx-_Tab5HUy",
      "metadata": {
        "id": "Okx-_Tab5HUy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fqggEBCu55Rh",
      "metadata": {
        "id": "fqggEBCu55Rh"
      },
      "source": [
        "Great advanced questions. Let‚Äôs break it down in 3 parts:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. **What Are the Parts of an Attention Layer?**\n",
        "\n",
        "In Transformer models, **each attention layer** has 3 core linear projections:\n",
        "\n",
        "| Component     | Role                             |\n",
        "| ------------- | -------------------------------- |\n",
        "| **Q (Query)** | What you‚Äôre looking for          |\n",
        "| **K (Key)**   | What‚Äôs available in the sentence |\n",
        "| **V (Value)** | The actual info you fetch        |\n",
        "\n",
        "Each token is turned into Q, K, V vectors.\n",
        "\n",
        "Then:\n",
        "\n",
        "```text\n",
        "Attention Score = softmax(Q √ó K·µÄ)\n",
        "Final Output    = Attention Score √ó V\n",
        "```\n",
        "\n",
        "So inside attention layers, you‚Äôll find:\n",
        "\n",
        "* `q_proj`: Layer that creates Queries\n",
        "* `k_proj`: Keys\n",
        "* `v_proj`: Values\n",
        "* `out_proj`: Final linear layer after attention\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **How Does LoRA Affect Existing Parameters?**\n",
        "\n",
        "Normally, fine-tuning updates all weights ‚Äî heavy, slow, and risky.\n",
        "\n",
        "LoRA **freezes** the original model weights. Instead:\n",
        "\n",
        "* It **injects tiny trainable layers** (called LoRA adapters)\n",
        "* These sit inside layers like `q_proj`, `v_proj`\n",
        "* During training, **only these adapters are updated**\n",
        "* At inference, output = original + adapter output\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "> üéí You're not rewriting the textbook (model weights) ‚Äî you're just writing notes (LoRA layers) on top of it.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **How Does LoRA Affect Model Performance?**\n",
        "\n",
        "| Feature            | Impact                                                                          |\n",
        "| ------------------ | ------------------------------------------------------------------------------- |\n",
        "| ‚úÖ **Memory/Speed** | Needs far less GPU RAM and time                                                 |\n",
        "| ‚úÖ **Modularity**   | You can plug/unplug behaviors (e.g., legal tone, sarcasm)                       |\n",
        "| ‚ö†Ô∏è **Accuracy**    | Slightly less powerful than full fine-tuning, but **excellent for small tasks** |\n",
        "| ‚úÖ **Low Cost**     | You can train on free Colab or 8GB GPU in many cases                            |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Summary: What LoRA Touches in Attention Layers\n",
        "\n",
        "You injected LoRA into:\n",
        "\n",
        "```python\n",
        "target_modules = [\"q_proj\", \"v_proj\"]\n",
        "```\n",
        "\n",
        "Which means you modified:\n",
        "\n",
        "* How the model **pays attention** (query-value attention)\n",
        "* Without changing the **core language knowledge**\n",
        "\n",
        "So it **learns new behaviors fast**, while staying stable.\n",
        "\n",
        "---\n",
        "\n",
        "Want to visualize LoRA in a block diagram next?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8zvY5IJO5-hw",
      "metadata": {
        "id": "8zvY5IJO5-hw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8rlAspDW9e77",
      "metadata": {
        "id": "8rlAspDW9e77"
      },
      "source": [
        "Great. Let's **deep dive into Attention Layers** ‚Äî the heart of Transformers like GPT, BERT, Mistral.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What is an Attention Layer?\n",
        "\n",
        "An **attention layer** allows the model to \"focus\" on different parts of a sentence when generating or understanding text.\n",
        "\n",
        "> It answers: **‚ÄúWhich words should I pay most attention to?‚Äù**\n",
        "\n",
        "---\n",
        "\n",
        "## üìê When is the Attention Layer Used?\n",
        "\n",
        "In every Transformer block, during both:\n",
        "\n",
        "* **Training** (learning patterns from text)\n",
        "* **Inference** (generating answers or predictions)\n",
        "\n",
        "It is used **repeatedly** ‚Äî across **each token**, and in **every layer**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Where Does the Attention Layer Fit in the Architecture?\n",
        "\n",
        "Here's the order inside a **Transformer block**:\n",
        "\n",
        "```\n",
        "Input Embedding\n",
        "‚Üì\n",
        "Position Encoding\n",
        "‚Üì\n",
        "üìç Multi-Head Attention Layer\n",
        "‚Üì\n",
        "Add & Norm\n",
        "‚Üì\n",
        "Feed Forward Layer (MLP)\n",
        "‚Üì\n",
        "Add & Norm\n",
        "‚Üì\n",
        "[Repeat next Transformer block or Output]\n",
        "```\n",
        "\n",
        "> Attention comes **before** the FeedForward layers and **after** embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ Internals: What Happens in an Attention Layer?\n",
        "\n",
        "Given an input sentence like:\n",
        "\n",
        "```txt\n",
        "\"The cat sat on the mat\"\n",
        "```\n",
        "\n",
        "### Step 1: Linear Projection\n",
        "\n",
        "Each token (word) embedding is transformed into 3 vectors:\n",
        "\n",
        "| Vector        | Meaning                       |\n",
        "| ------------- | ----------------------------- |\n",
        "| **Query (Q)** | What this word is looking for |\n",
        "| **Key (K)**   | What this word offers         |\n",
        "| **Value (V)** | The actual content/info       |\n",
        "\n",
        "So every word has:\n",
        "\n",
        "* A **Q**: What does it care about?\n",
        "* A **K**: Should others care about me?\n",
        "* A **V**: What info do I carry?\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Scoring\n",
        "\n",
        "Now compute how much attention a word should pay to every other word.\n",
        "\n",
        "```python\n",
        "score = softmax(Q √ó K·µÄ)\n",
        "```\n",
        "\n",
        "This gives a score matrix ‚Äî how much each word attends to others.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Weighted Sum\n",
        "\n",
        "Use these scores to fetch the right info:\n",
        "\n",
        "```python\n",
        "output = score √ó V\n",
        "```\n",
        "\n",
        "This gives you the new representation of the word ‚Äî based on **context**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Why is Attention So Powerful?\n",
        "\n",
        "* üîç **Context Awareness**: Word meaning changes by context. Attention captures that.\n",
        "* üîÅ **Long-Range Understanding**: It can relate \"cat\" to \"mat\" even if they are far apart.\n",
        "* üß† **Core of LLMs**: Transformers = just many stacked attention + feedforward blocks.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Example: The word \"bank\"\n",
        "\n",
        "> In: \"He sat by the bank of the river.\"\n",
        "\n",
        "The Q vector for \"bank\" may score high attention to \"river\", not \"money\".\n",
        "\n",
        "---\n",
        "\n",
        "## üîó What Comes Before and After?\n",
        "\n",
        "| Layer               | Role                                              |\n",
        "| ------------------- | ------------------------------------------------- |\n",
        "| **Before**          | Word embeddings + position encoding               |\n",
        "| **Attention Layer** | Computes weighted focus                           |\n",
        "| **After**           | MLP (FeedForward Layer) ‚Üí processes attended info |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Recap: What Does Attention Layer Do?\n",
        "\n",
        "* Projects input into Q, K, V\n",
        "* Computes how much to attend to other tokens\n",
        "* Recomputes a better version of the token using this context\n",
        "\n",
        "This process is repeated in **every transformer block**, many times over.\n",
        "\n",
        "---\n",
        "\n",
        "Want a visual diagram next or dive into **Multi-Head Attention**?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "GhHA0fp39frd",
      "metadata": {
        "id": "GhHA0fp39frd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "_tTQ4BtR_Rew",
      "metadata": {
        "id": "_tTQ4BtR_Rew"
      },
      "source": [
        "Perfect. Let‚Äôs break down **how a Transformer model works step-by-step** when a **user sends a request** (like a question or sentence):\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Step-by-Step: What Happens When a Request Is Made\n",
        "\n",
        "### üßæ Input: \"What is the capital of India?\"\n",
        "\n",
        "You send this to a model like GPT or Mistral.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ Step 1: **Tokenization**\n",
        "\n",
        "* The sentence is split into **tokens**:\n",
        "\n",
        "  ```\n",
        "  [\"What\", \"is\", \"the\", \"capital\", \"of\", \"India\", \"?\"]\n",
        "  ```\n",
        "* Each token is converted to an integer ID (using a vocab).\n",
        "\n",
        "  ```\n",
        "  [1021, 56, 11, 9821, 23, 4569, 35]\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### üî° Step 2: **Embedding Layer**\n",
        "\n",
        "* Each token ID is converted to a **dense vector** (like a 768-dim vector):\n",
        "\n",
        "  ```\n",
        "  [0.1, 0.6, ..., 0.02]  ‚Üê for each word\n",
        "  ```\n",
        "\n",
        "* Adds **positional encoding** ‚Üí tells the model the order of words.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Step 3: **Transformer Blocks (Repeated \\~12‚Äì48 times)**\n",
        "\n",
        "Each block =\n",
        "\n",
        "> **Multi-Head Attention** ‚Üí Add & Norm ‚Üí Feed Forward ‚Üí Add & Norm\n",
        "\n",
        "Let‚Äôs go through one block:\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç 3.1 Multi-Head Attention\n",
        "\n",
        "* Each word is mapped to **Query (Q)**, **Key (K)**, **Value (V)** vectors.\n",
        "\n",
        "* Attention scores are computed between every pair of words:\n",
        "\n",
        "  * \"capital\" attends most to \"India\"\n",
        "  * \"what\" attends most to \"capital\"\n",
        "\n",
        "* Output: Each word‚Äôs vector is now **context-aware**.\n",
        "\n",
        "> It knows what other words matter most.\n",
        "\n",
        "---\n",
        "\n",
        "#### üßÆ 3.2 Feed Forward Network (MLP)\n",
        "\n",
        "* Each context-aware vector goes through a small neural net.\n",
        "* This helps model complex patterns (like grammar, logic, etc).\n",
        "\n",
        "---\n",
        "\n",
        "#### üîÅ 3.3 Repeat for Multiple Layers\n",
        "\n",
        "* The output of Block 1 becomes input to Block 2.\n",
        "* Each layer learns deeper language features:\n",
        "\n",
        "  * Layer 1: grammar\n",
        "  * Layer 4: relationships\n",
        "  * Layer 10: logic\n",
        "  * Layer 30: reasoning\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Step 4: **Final Output Layer**\n",
        "\n",
        "* Final vector goes to a **linear layer + softmax**\n",
        "* Outputs **next-token prediction**:\n",
        "\n",
        "  * Based on everything it saw, what word should come next?\n",
        "\n",
        "E.g.,\n",
        "\n",
        "```\n",
        "\"What is the capital of India?\" ‚Üí \"New\"\n",
        "Then ‚Üí \"Delhi\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Summary of Flow\n",
        "\n",
        "```\n",
        "Input text ‚Üí Tokenize ‚Üí Embed ‚Üí Transformer Blocks ‚Üí Next Token Output\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí• What‚Äôs Special About This Flow?\n",
        "\n",
        "* It‚Äôs **parallelizable**: All words processed together.\n",
        "* Each layer captures different **depth of understanding**.\n",
        "* Uses **only attention** ‚Äî no RNN or CNN.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you want:\n",
        "\n",
        "* Visual flowchart of this\n",
        "* How generation happens token-by-token\n",
        "* Or deep dive into **multi-head attention**\n",
        "\n",
        "Ready?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RpfE79BV_SBz",
      "metadata": {
        "id": "RpfE79BV_SBz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "NUWKwYhI_tJE",
      "metadata": {
        "id": "NUWKwYhI_tJE"
      },
      "source": [
        "Great ‚Äî let‚Äôs come back to **LoRA (Low-Rank Adaptation)** and understand it fully in the context of Transformers and fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What is LoRA?\n",
        "\n",
        "LoRA is a **lightweight fine-tuning technique** that **adds small trainable adapter layers** to a frozen base model.\n",
        "\n",
        "Instead of updating millions (or billions) of weights, it:\n",
        "\n",
        "* **Freezes the original model**\n",
        "* Adds **small rank-decomposed matrices** inside attention layers\n",
        "* **Only trains those**\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Where Exactly Does LoRA Apply?\n",
        "\n",
        "Inside each **Transformer block**, there‚Äôs a **self-attention mechanism**.\n",
        "\n",
        "Each attention head uses:\n",
        "\n",
        "```text\n",
        "Q = X √ó W_q\n",
        "K = X √ó W_k\n",
        "V = X √ó W_v\n",
        "```\n",
        "\n",
        "LoRA **injects adapters** into these linear projections, like `W_q` or `W_v`:\n",
        "\n",
        "```text\n",
        "W_q ‚Üê W_q (frozen) + A √ó B  (LoRA adapters)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `A` is a small down-projection (e.g., 768 ‚Üí 8)\n",
        "* `B` is an up-projection (8 ‚Üí 768)\n",
        "\n",
        "These tiny matrices are trainable ‚Äî rest is frozen.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è What LoRA Parameters Mean\n",
        "\n",
        "```python\n",
        "peft_config = LoraConfig(\n",
        "    r=8,                        # Rank of A/B (the size of bottleneck)\n",
        "    lora_alpha=16,              # Scaling factor applied to output\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Inject into Q and V projections\n",
        "    lora_dropout=0.05,          # Dropout for LoRA path (regularization)\n",
        "    bias=\"none\",                # Whether to train bias too\n",
        "    task_type=TaskType.CAUSAL_LM  # Specifies this is a language model\n",
        ")\n",
        "```\n",
        "\n",
        "### üîß What Each Does:\n",
        "\n",
        "| Parameter        | Purpose                                                                 |\n",
        "| ---------------- | ----------------------------------------------------------------------- |\n",
        "| `r`              | The size of the adapter (controls memory/speed vs performance tradeoff) |\n",
        "| `alpha`          | Scales the adapter output, stabilizing training                         |\n",
        "| `target_modules` | Which parts of the model to insert LoRA into                            |\n",
        "| `dropout`        | Regularization to avoid overfitting                                     |\n",
        "| `bias`           | Whether to also train bias terms (usually \"none\" or \"all\")              |\n",
        "| `task_type`      | Helps PEFT know which forward() method to use                           |\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Why Use LoRA?\n",
        "\n",
        "| Without LoRA                   | With LoRA                              |\n",
        "| ------------------------------ | -------------------------------------- |\n",
        "| Train full model (slow, heavy) | Freeze model, train small adapters     |\n",
        "| Needs lots of GPU + time       | Runs on low RAM & can fit on Colab     |\n",
        "| Hard to deploy (large)         | Small diff files (\\~5MB) can be merged |\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Does It Affect Performance?\n",
        "\n",
        "* ‚úÖ Almost **same or better** performance on many tasks.\n",
        "* üöÄ **Faster training**, even on 1 GPU.\n",
        "* üîß Easy to remove adapters ‚Üí original model stays unchanged.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you want:\n",
        "\n",
        "* A visual showing where LoRA plugs in\n",
        "* How merging works (`merge_and_unload()`)\n",
        "* Or comparison with full fine-tuning\n",
        "\n",
        "Shall we move to **evaluation or saving LoRA model** next?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ipdtzPn4_tjQ",
      "metadata": {
        "id": "ipdtzPn4_tjQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "LZ6P2FEyCFUm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ6P2FEyCFUm",
        "outputId": "9b82aeb9-e26f-47ba-9186-95996db1dbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes --prefer-binary --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CCwagaYTCHAp",
      "metadata": {
        "id": "CCwagaYTCHAp"
      },
      "source": [
        "### ‚úÖ What is QLoRA?\n",
        "\n",
        "**QLoRA** = Quantized Low-Rank Adapter.\n",
        "It‚Äôs just like LoRA but optimized for low memory usage using quantized models (like 4-bit instead of 16-bit).\n",
        "\n",
        "This means:\n",
        "\n",
        "* You can fine-tune large models (7B, 13B) even on a single GPU (or Colab).\n",
        "* It combines **4-bit quantization** (to save memory) + **LoRA adapters** (to train efficiently).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "HEdQrPxBCFLA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "11ac0cac00b640569edbf36e3492bb74",
            "41d6be5f40de449c9acd6c948f88f36f",
            "a2f70bf212ea4845a295f05d91116778",
            "a26f06aa734b4441b799c3e6599392ad",
            "c9fafd1d6645451e89713456434e6549",
            "fc28946e9ddf4d2a9f56f4b12d3613aa",
            "f142168966be4b1b9981ced5577a1c42",
            "5fdcac4f26ee4bcd945818e612808a1c",
            "1ef73879ba0f432f9a39dfe1aca97ce3",
            "4e033dd898df45008f8022991c2350a8",
            "7fb32c28327647fcb9785df420d88dcf",
            "ec06c050cb4a4309a9a2db57a1ee677b",
            "f1433bcbfac443089673710a1468d149",
            "e01430e606be4afaaf2d64eac34209e6",
            "232185002d4848ee846a924286ff58a8",
            "c315a79089804f63ad1c8d134ce6429a",
            "1fc0b42d1bd14bc5b7c5a89be6fe6184",
            "f968a68f41714966a69fac3d235dae23",
            "5cda90f6db7041d6a187c95086ac4530",
            "61b2b00d880e4888a77161d4444344d6",
            "76d51391d31744f6b31e8cd7fbd3dc43",
            "07520aba8f4541f78163509035d38137"
          ]
        },
        "id": "HEdQrPxBCFLA",
        "outputId": "4bcb0c9c-2508-4751-b264-95dc657f0bdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11ac0cac00b640569edbf36e3492bb74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec06c050cb4a4309a9a2db57a1ee677b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "IEUhcjIgCfav",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEUhcjIgCfav",
        "outputId": "bbbec403-7937-4a46-e7f1-a4e68853a4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "sf19narkCxGs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "def26cb3369440c89aa9237099c88eec",
            "80802c9b286d42f2bff51734d43a3262",
            "d8e41a4e8b964e9eaa84fa32ca5a0e54",
            "146f80b321704b748e0902471bf16b0b",
            "1ac5ab25660b40d898fd26159d2374d9",
            "fbd29a77c3e04332b8f59cefdca1cff6",
            "23be24d7b1914862b833b0af6a11e363",
            "73429e8456534f9184f8878b67863286",
            "5139be6e1d2240c38dea4e4896edc3d7",
            "1e10aea07c174039badbbc67b30a4a5a",
            "13db585e9ab04535bc1beebf35f5c424",
            "25439f5e0d314b0e83c468e11c6cc2fe",
            "68330820d05c445d93f5ac84e363bc73",
            "e6efda6d466f4b3fba59f3b8834d5646",
            "0958e8727ff841d590ba0697beed78e2",
            "0198f9c49058461d938d25e663a6dfd6",
            "c6a257f987634ea7af57c336a6c0554e",
            "f1f2ac70a2a84b87a13d07842725925e",
            "919e63b6848640c5bfda7d6ccb412321",
            "b52213a9859c4f3d8731eade5278c8c7",
            "2e3741895ab04ff0868e10fd511480b0",
            "c69d66bcaddd4a298c87b81902db78f6",
            "30f2cbd177af4056adedd1cbb1802978",
            "b68ac91688174348b416133779baad56",
            "ac6afec2be56481899342bece8f405b5",
            "e35f828cbf7b4e28907221cf31293115",
            "76a608ecca374f628ce34d551aa89a03",
            "93bd0bf473584c608ab0e717acd719a1",
            "0271970357464a908e8f3cacac68369c",
            "fa14e496f4b54a84ba3dc8efc509506d",
            "860ae1d6edd249b8a993d46cacef4e98",
            "d9cb1daa061e4582a753a61621f94f38",
            "f652638eb7f24fe4832c8718990848e7"
          ]
        },
        "id": "sf19narkCxGs",
        "outputId": "b4a99946-a793-4dcf-b4cc-067b4fb001c1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "def26cb3369440c89aa9237099c88eec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25439f5e0d314b0e83c468e11c6cc2fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30f2cbd177af4056adedd1cbb1802978",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ‚úÖ Step 4: Prepare Dataset for QLoRA Fine-Tuning\n",
        "# We‚Äôll use the same Alpaca-style dataset, but this time you‚Äôll format and tokenize it properly for QLoRA training.\n",
        "\n",
        "# üßæ 1. Format Alpaca Prompts\n",
        "# You may already have a dataset like this:\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "# Load the same data\n",
        "with open(\"sample_alpaca.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "def format_prompt(example):\n",
        "    if example[\"input\"]:\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Input:\n",
        "{example['input']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "\n",
        "dataset = dataset.map(lambda x: {\"text\": format_prompt(x)})\n",
        "\n",
        "# üß™ 2. Tokenize Dataset with Padding/Truncation\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# ‚úÖ Add labels (required for Trainer)\n",
        "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
        "# ‚úÖ This will prepare the dataset for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "hvczgbF3DZ_R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvczgbF3DZ_R",
        "outputId": "d8c716a9-a765-473d-ece2-b74960820b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ 3. Load the Model with 4-bit Quantization\n",
        "#üîß Load a 4-bit Quantized Model with BitsAndBytes\n",
        "# We‚Äôll use BitsAndBytesConfig to load the model in 4-bit precision, which is memory efficient.\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",        # Better performance\n",
        "    bnb_4bit_use_double_quant=True,   # Extra compression\n",
        "    bnb_4bit_compute_dtype=\"float16\" # More stable than float16 # or torch.float16\n",
        ")\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# üß™ 4. Apply LoRA via PEFT\n",
        "# Now that you‚Äôve successfully loaded the model in 4-bit using BitsAndBytesConfig\n",
        "# ‚úÖ Apply QLoRA Adapter via PEFT\n",
        "# We‚Äôll now wrap the quantized model using PEFT‚Äôs LoraConfig ‚Äî this is QLoRA (LoRA on quantized models).\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # works well for transformer models\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# üîç This step ensures:\n",
        "# You're only training lightweight LoRA layers on top of a frozen 4-bit model\n",
        "# Saves massive GPU/CPU RAM and enables fine-tuning 1B+ models on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "LKsxda3Oaw5R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "LKsxda3Oaw5R",
        "outputId": "8fc5bde8-0616-4fba-9ad9-6ee2c4906609"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=16.74243418375651, metrics={'train_runtime': 3.0581, 'train_samples_per_second': 4.905, 'train_steps_per_second': 1.962, 'total_flos': 47722235166720.0, 'train_loss': 16.74243418375651, 'epoch': 3.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ‚úÖ 4. Training Setup\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_qlora\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "# ‚úÖ 5. Start Training\n",
        "trainer.train() #üìù This will do QLoRA-based fine-tuning. It's lightweight and works even on T4 GPU on Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bLZp-pxqbiZ0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLZp-pxqbiZ0",
        "outputId": "2abbb847-d5f3-485e-945d-3767cbbf2acb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Convert to binary\n",
            "\n",
            "### Input:\n",
            "7\n",
            "\n",
            "### Response:\n",
            "101\n",
            "```\n",
            "\n",
            "The program prompts the user to enter a decimal number, converts it to binary, and then outputs the binary representation. The `>>` operator is used to read input from the user, and the `format\n"
          ]
        }
      ],
      "source": [
        "# 1. üß™ Test Your Fine-Tuned Model\n",
        "# Use a simple prompt to test it:\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"\"\"### Instruction:\n",
        "Convert to binary\n",
        "\n",
        "### Input:\n",
        "7\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "response = pipe(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
        "print(response[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ilNLBKj-dEM_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilNLBKj-dEM_",
        "outputId": "af5db3ac-2a8c-4fc1-c8d0-04d2c9884471"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('qlora-tinyllama/tokenizer_config.json',\n",
              " 'qlora-tinyllama/special_tokens_map.json',\n",
              " 'qlora-tinyllama/chat_template.jinja',\n",
              " 'qlora-tinyllama/tokenizer.model',\n",
              " 'qlora-tinyllama/added_tokens.json',\n",
              " 'qlora-tinyllama/tokenizer.json')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. üíæ Save & Reload Your Model (Locally or to Hugging Face)\n",
        "model.save_pretrained(\"qlora-tinyllama\")\n",
        "tokenizer.save_pretrained(\"qlora-tinyllama\")\n",
        "\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "\n",
        "# model.push_to_hub(\"your-username/qlora-tinyllama\")\n",
        "# tokenizer.push_to_hub(\"your-username/qlora-tinyllama\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h-duhFFBdKiW",
      "metadata": {
        "id": "h-duhFFBdKiW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "HXj7fW5edfG0",
      "metadata": {
        "id": "HXj7fW5edfG0"
      },
      "source": [
        "‚úÖ QLoRA Fine-Tuning Completed Successfully!\n",
        "\n",
        "You've now fully completed fine-tuning TinyLlama with QLoRA adapters on your Alpaca dataset.\n",
        "\n",
        "üîç What just happened?\n",
        "You trained only the lightweight LoRA layers (not the full model).\n",
        "\n",
        "Your final loss: 16.74 ‚Üí For such a tiny dataset, that‚Äôs expected.\n",
        "\n",
        "Training ran in < 5 seconds on GPU ‚Äî that‚Äôs the power of QLoRA.\n",
        "\n",
        "\n",
        "3. üöÄ Use in Real Projects\n",
        "You can now plug this into:\n",
        "\n",
        "- Chat UI (Streamlit, Gradio)\n",
        "- Agents (LangChain, OpenRouter tools)\n",
        "- CLI assistant or automation tasks\n",
        "\n",
        "\n",
        "- üß™ Test the model\n",
        "- üíæ Save it\n",
        "- üì§ Upload to Hugging Face\n",
        "- üß† Train on bigger dataset (e.g., - full Alpaca)\n",
        "- üöÄ Use in chatbot or assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fKgEdwIriagS",
      "metadata": {
        "id": "fKgEdwIriagS"
      },
      "source": [
        "Great question ‚Äî this is **core foundational knowledge** for working with LLM fine-tuning in 2025.\n",
        "\n",
        "Let‚Äôs break it down simply and clearly:\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What is LoRA?\n",
        "\n",
        "**LoRA = Low-Rank Adaptation**\n",
        "\n",
        "> Instead of updating *all* the billions of parameters in a large model, LoRA freezes the base model and only **adds a small number of trainable weights** to specific layers (usually attention layers like `q_proj` and `v_proj`).\n",
        "\n",
        "‚úÖ **Why use LoRA?**\n",
        "\n",
        "* **Saves memory**\n",
        "* **Much faster** to train\n",
        "* You can fine-tune big models (7B+) on a laptop/GPU\n",
        "\n",
        "---\n",
        "\n",
        "### üî¨ What is QLoRA?\n",
        "\n",
        "**QLoRA = Quantized + LoRA**\n",
        "\n",
        "> QLoRA builds on LoRA by also **loading the base model in 4-bit (very compressed)** ‚Äî using a library called **`bitsandbytes`**.\n",
        "\n",
        "This gives **even more memory savings**.\n",
        "\n",
        "| Feature         | LoRA           | QLoRA                                 |\n",
        "| --------------- | -------------- | ------------------------------------- |\n",
        "| Model precision | 16-bit / 8-bit | **4-bit** (via quantization)          |\n",
        "| Base model size | Medium         | **Tiny** (same model, but compressed) |\n",
        "| RAM usage       | Low            | **Ultra low**                         |\n",
        "| Training speed  | Fast           | **Faster**                            |\n",
        "| Accuracy        | ‚úÖ Good         | ‚úÖ Slight drop, but great overall      |\n",
        "\n",
        "---\n",
        "\n",
        "### üß© What is PEFT?\n",
        "\n",
        "**PEFT = Parameter-Efficient Fine-Tuning**\n",
        "\n",
        "LoRA and QLoRA are **types of PEFT**.\n",
        "There are others too: Prompt Tuning, Adapter Tuning, etc.\n",
        "\n",
        "The **`peft` library by Hugging Face** helps you:\n",
        "\n",
        "* Apply LoRA/QLoRA easily\n",
        "* Manage which weights are frozen/trainable\n",
        "* Save only adapter weights\n",
        "* Load + merge adapters into base model\n",
        "\n",
        "‚úÖ So, when you see:\n",
        "\n",
        "```python\n",
        "from peft import LoraConfig, get_peft_model\n",
        "```\n",
        "\n",
        "That‚Äôs PEFT in action!\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Summary:\n",
        "\n",
        "* **LoRA**: Only trains small adapter weights ‚Üí saves time/memory\n",
        "* **QLoRA**: Same idea + compresses model to 4-bit ‚Üí trains on laptops\n",
        "* **PEFT**: The framework that lets you do both easily\n",
        "\n",
        "Let me know if you want a diagram or a project recommendation next.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jcq_FQLddfhV",
      "metadata": {
        "id": "Jcq_FQLddfhV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "H4gM30jfi4kl",
      "metadata": {
        "id": "H4gM30jfi4kl"
      },
      "source": [
        "Great! Now that you've completed fine-tuning with a small dataset using both **LoRA** and **QLoRA**, you're ready for the next step: **training on a larger dataset**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Here‚Äôs the plan to scale up:\n",
        "\n",
        "#### **Step 1: Choose a Larger Dataset**\n",
        "\n",
        "You can use public datasets from Hugging Face like:\n",
        "\n",
        "* [`tatsu-lab/alpaca`](https://huggingface.co/datasets/tatsu-lab/alpaca)\n",
        "* [`timdettmers/openassistant-guanaco`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)\n",
        "* [`yahma/alpaca-cleaned`](https://huggingface.co/datasets/yahma/alpaca-cleaned)\n",
        "\n",
        "Let me know which one you want to use, or I‚Äôll pick one for you.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 2: Load & Format It**\n",
        "\n",
        "You‚Äôll use `load_dataset()` from `datasets`, then apply the same `format_prompt()` logic to convert it into instruction format.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Tokenize and Train**\n",
        "\n",
        "Use the same steps:\n",
        "\n",
        "* Tokenize with `AutoTokenizer`\n",
        "* Apply LoRA/QLoRA\n",
        "* Fine-tune using `Trainer`\n",
        "\n",
        "---\n",
        "\n",
        "Do you want to proceed with:\n",
        "\n",
        "* üîπ `tatsu-lab/alpaca` (52k instructions)\n",
        "* üîπ `yahma/alpaca-cleaned` (cleaned, same 52k)\n",
        "* üîπ `openassistant-guanaco` (larger, \\~10x bigger)\n",
        "\n",
        "Let me know your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9cJ11Gzi5Bo",
      "metadata": {
        "id": "b9cJ11Gzi5Bo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Hoi78ZF9jEzn",
      "metadata": {
        "id": "Hoi78ZF9jEzn"
      },
      "source": [
        "Great! Now that you've completed fine-tuning with a small dataset using both **LoRA** and **QLoRA**, you're ready for the next step: **training on a larger dataset**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Here‚Äôs the plan to scale up:\n",
        "\n",
        "#### **Step 1: Choose a Larger Dataset**\n",
        "\n",
        "You can use public datasets from Hugging Face like:\n",
        "\n",
        "* [`tatsu-lab/alpaca`](https://huggingface.co/datasets/tatsu-lab/alpaca)\n",
        "* [`timdettmers/openassistant-guanaco`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)\n",
        "* [`yahma/alpaca-cleaned`](https://huggingface.co/datasets/yahma/alpaca-cleaned)\n",
        "\n",
        "Let me know which one you want to use, or I‚Äôll pick one for you.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 2: Load & Format It**\n",
        "\n",
        "You‚Äôll use `load_dataset()` from `datasets`, then apply the same `format_prompt()` logic to convert it into instruction format.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Tokenize and Train**\n",
        "\n",
        "Use the same steps:\n",
        "\n",
        "* Tokenize with `AutoTokenizer`\n",
        "* Apply LoRA/QLoRA\n",
        "* Fine-tune using `Trainer`\n",
        "\n",
        "---\n",
        "\n",
        "Do you want to proceed with:\n",
        "\n",
        "* üîπ `tatsu-lab/alpaca` (52k instructions)\n",
        "* üîπ `yahma/alpaca-cleaned` (cleaned, same 52k)\n",
        "* üîπ `openassistant-guanaco` (larger, \\~10x bigger)\n",
        "\n",
        "Let me know your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wd_0KvW5jFBY",
      "metadata": {
        "id": "Wd_0KvW5jFBY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0198f9c49058461d938d25e663a6dfd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0271970357464a908e8f3cacac68369c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "037a5d4b998a4236b207ea0b0dc56712": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06c522a629924d2ab28cac95093f084a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2fe5b15027c412f9c874fef838a2bf8",
              "IPY_MODEL_f71f3644c615407cabb67993b93d99d9",
              "IPY_MODEL_fc19acd2bfdf4f36aff0f7c7c6a5ba7d"
            ],
            "layout": "IPY_MODEL_df22c95132404309af5bc91a2a4c5f35"
          }
        },
        "07520aba8f4541f78163509035d38137": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0958e8727ff841d590ba0697beed78e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e3741895ab04ff0868e10fd511480b0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c69d66bcaddd4a298c87b81902db78f6",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá206.39‚Äáexamples/s]"
          }
        },
        "11ac0cac00b640569edbf36e3492bb74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41d6be5f40de449c9acd6c948f88f36f",
              "IPY_MODEL_a2f70bf212ea4845a295f05d91116778",
              "IPY_MODEL_a26f06aa734b4441b799c3e6599392ad"
            ],
            "layout": "IPY_MODEL_c9fafd1d6645451e89713456434e6549"
          }
        },
        "13db585e9ab04535bc1beebf35f5c424": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "146f80b321704b748e0902471bf16b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e10aea07c174039badbbc67b30a4a5a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_13db585e9ab04535bc1beebf35f5c424",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá293.56‚Äáexamples/s]"
          }
        },
        "1ac5ab25660b40d898fd26159d2374d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e10aea07c174039badbbc67b30a4a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ef73879ba0f432f9a39dfe1aca97ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fc0b42d1bd14bc5b7c5a89be6fe6184": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2256b603cd234f2ea1162fcac670e1e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "232185002d4848ee846a924286ff58a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d51391d31744f6b31e8cd7fbd3dc43",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_07520aba8f4541f78163509035d38137",
            "value": "‚Äá124/124‚Äá[00:00&lt;00:00,‚Äá12.8kB/s]"
          }
        },
        "23be24d7b1914862b833b0af6a11e363": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25439f5e0d314b0e83c468e11c6cc2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68330820d05c445d93f5ac84e363bc73",
              "IPY_MODEL_e6efda6d466f4b3fba59f3b8834d5646",
              "IPY_MODEL_0958e8727ff841d590ba0697beed78e2"
            ],
            "layout": "IPY_MODEL_0198f9c49058461d938d25e663a6dfd6"
          }
        },
        "28db79e180c54dccb35605429b5d2ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "294f4c44d8ff4e8296e51c1ff252df48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e3741895ab04ff0868e10fd511480b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30f2cbd177af4056adedd1cbb1802978": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b68ac91688174348b416133779baad56",
              "IPY_MODEL_ac6afec2be56481899342bece8f405b5",
              "IPY_MODEL_e35f828cbf7b4e28907221cf31293115"
            ],
            "layout": "IPY_MODEL_76a608ecca374f628ce34d551aa89a03"
          }
        },
        "318977280bf8482eb693e24c1c3540ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa999179ef34883ade4dc4f52d31170": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f290ea229c460da2101c0031fef80a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc6086264901457482a00d56793eb551",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá100.18‚Äáexamples/s]"
          }
        },
        "41d6be5f40de449c9acd6c948f88f36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc28946e9ddf4d2a9f56f4b12d3613aa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f142168966be4b1b9981ced5577a1c42",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "49fffde822ee4c04bc3a2f62679f97a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a688b8454bc479289f03228d962ea71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_815a2447343c4d168bf98659e3c8ba16",
              "IPY_MODEL_de17459f594540b0be050ad4304ee422",
              "IPY_MODEL_3aa999179ef34883ade4dc4f52d31170"
            ],
            "layout": "IPY_MODEL_318977280bf8482eb693e24c1c3540ff"
          }
        },
        "4e033dd898df45008f8022991c2350a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5139be6e1d2240c38dea4e4896edc3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "538f7f2354fe49408d444d61edd53465": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91311cf847c041bab4f771ec64efb700",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e81de64a92ef4813b68e238b4bc1cfb2",
            "value": "Map:‚Äá100%"
          }
        },
        "579054c6790c4238be2bc2ff4b569297": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cda90f6db7041d6a187c95086ac4530": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eb22dfd40744855a47204f136204953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f6fddd65e95466ea5e92e483e7c4f26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fdcac4f26ee4bcd945818e612808a1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61b2b00d880e4888a77161d4444344d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68330820d05c445d93f5ac84e363bc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6a257f987634ea7af57c336a6c0554e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f1f2ac70a2a84b87a13d07842725925e",
            "value": "Map:‚Äá100%"
          }
        },
        "6d7970be944b40be81f2218b57fe1486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c422a0ff083e45a6bd7162f6a637552f",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5eb22dfd40744855a47204f136204953",
            "value": 5
          }
        },
        "73429e8456534f9184f8878b67863286": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a608ecca374f628ce34d551aa89a03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76d51391d31744f6b31e8cd7fbd3dc43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc3d986d00841ada6e9cb437f2dd482": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fb32c28327647fcb9785df420d88dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80802c9b286d42f2bff51734d43a3262": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbd29a77c3e04332b8f59cefdca1cff6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_23be24d7b1914862b833b0af6a11e363",
            "value": "Map:‚Äá100%"
          }
        },
        "815a2447343c4d168bf98659e3c8ba16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac4f107127d7473ab7d6315102f6ad16",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49fffde822ee4c04bc3a2f62679f97a2",
            "value": "Map:‚Äá100%"
          }
        },
        "860ae1d6edd249b8a993d46cacef4e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b819112bdb7474490a8d8fc7ad1756b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037a5d4b998a4236b207ea0b0dc56712",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b7a18bff47564bb286567c674ba3429c",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá168.62‚Äáexamples/s]"
          }
        },
        "8c588711ce494c17a2e4bcdd270c08fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91311cf847c041bab4f771ec64efb700": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919e63b6848640c5bfda7d6ccb412321": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93bd0bf473584c608ab0e717acd719a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9430c83e78e34ef38bb5df9d188a2ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_538f7f2354fe49408d444d61edd53465",
              "IPY_MODEL_6d7970be944b40be81f2218b57fe1486",
              "IPY_MODEL_b5ad66532b9c43b69462d28da53a1ab9"
            ],
            "layout": "IPY_MODEL_294f4c44d8ff4e8296e51c1ff252df48"
          }
        },
        "96c42403ee5048578f5ad123538362f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c44b73b4f504b6594444d1fc0609d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ddc08baefdd4e8f96bb98354ac20bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a26f06aa734b4441b799c3e6599392ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e033dd898df45008f8022991c2350a8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7fb32c28327647fcb9785df420d88dcf",
            "value": "‚Äá2.20G/2.20G‚Äá[00:27&lt;00:00,‚Äá136MB/s]"
          }
        },
        "a2f70bf212ea4845a295f05d91116778": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fdcac4f26ee4bcd945818e612808a1c",
            "max": 2200119864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ef73879ba0f432f9a39dfe1aca97ce3",
            "value": 2200119864
          }
        },
        "a36a2ac89226489e8bca6865d0dae14d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fda97e1770e94ac78bd7a3a06e910090",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b54f587ce5dc4b3298c7e42d1eea711f",
            "value": "Map:‚Äá100%"
          }
        },
        "ac4f107127d7473ab7d6315102f6ad16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac6afec2be56481899342bece8f405b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa14e496f4b54a84ba3dc8efc509506d",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_860ae1d6edd249b8a993d46cacef4e98",
            "value": 5
          }
        },
        "b52213a9859c4f3d8731eade5278c8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b54f587ce5dc4b3298c7e42d1eea711f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5ad66532b9c43b69462d28da53a1ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_579054c6790c4238be2bc2ff4b569297",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ddc08baefdd4e8f96bb98354ac20bfb",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá159.68‚Äáexamples/s]"
          }
        },
        "b5e9a2b001af452aabaad9c55a423f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b68ac91688174348b416133779baad56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93bd0bf473584c608ab0e717acd719a1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0271970357464a908e8f3cacac68369c",
            "value": "Map:‚Äá100%"
          }
        },
        "b7a18bff47564bb286567c674ba3429c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10812a919974c90bcf577c7f6c1fb34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a36a2ac89226489e8bca6865d0dae14d",
              "IPY_MODEL_ed41ca9e45e84ac997cd30652a3a2bdf",
              "IPY_MODEL_8b819112bdb7474490a8d8fc7ad1756b"
            ],
            "layout": "IPY_MODEL_96c42403ee5048578f5ad123538362f0"
          }
        },
        "c2d5c55557154b6aab681d274c9e60c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c315a79089804f63ad1c8d134ce6429a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c422a0ff083e45a6bd7162f6a637552f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c69d66bcaddd4a298c87b81902db78f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a257f987634ea7af57c336a6c0554e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9fafd1d6645451e89713456434e6549": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2fe5b15027c412f9c874fef838a2bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc7ed83cff4f4aaba55f4688e21a27d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b5e9a2b001af452aabaad9c55a423f6c",
            "value": "Map:‚Äá100%"
          }
        },
        "d3ea4589e0f0401a9f5fbcd365ab97d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8e41a4e8b964e9eaa84fa32ca5a0e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73429e8456534f9184f8878b67863286",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5139be6e1d2240c38dea4e4896edc3d7",
            "value": 5
          }
        },
        "d9cb1daa061e4582a753a61621f94f38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6086264901457482a00d56793eb551": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de17459f594540b0be050ad4304ee422": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f6fddd65e95466ea5e92e483e7c4f26",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d5c55557154b6aab681d274c9e60c5",
            "value": 5
          }
        },
        "def26cb3369440c89aa9237099c88eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80802c9b286d42f2bff51734d43a3262",
              "IPY_MODEL_d8e41a4e8b964e9eaa84fa32ca5a0e54",
              "IPY_MODEL_146f80b321704b748e0902471bf16b0b"
            ],
            "layout": "IPY_MODEL_1ac5ab25660b40d898fd26159d2374d9"
          }
        },
        "df22c95132404309af5bc91a2a4c5f35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01430e606be4afaaf2d64eac34209e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cda90f6db7041d6a187c95086ac4530",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61b2b00d880e4888a77161d4444344d6",
            "value": 124
          }
        },
        "e35f828cbf7b4e28907221cf31293115": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9cb1daa061e4582a753a61621f94f38",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f652638eb7f24fe4832c8718990848e7",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá184.00‚Äáexamples/s]"
          }
        },
        "e3f290ea229c460da2101c0031fef80a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6efda6d466f4b3fba59f3b8834d5646": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919e63b6848640c5bfda7d6ccb412321",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b52213a9859c4f3d8731eade5278c8c7",
            "value": 5
          }
        },
        "e81de64a92ef4813b68e238b4bc1cfb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec06c050cb4a4309a9a2db57a1ee677b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1433bcbfac443089673710a1468d149",
              "IPY_MODEL_e01430e606be4afaaf2d64eac34209e6",
              "IPY_MODEL_232185002d4848ee846a924286ff58a8"
            ],
            "layout": "IPY_MODEL_c315a79089804f63ad1c8d134ce6429a"
          }
        },
        "ed41ca9e45e84ac997cd30652a3a2bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3ea4589e0f0401a9f5fbcd365ab97d4",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2256b603cd234f2ea1162fcac670e1e7",
            "value": 5
          }
        },
        "f142168966be4b1b9981ced5577a1c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1433bcbfac443089673710a1468d149": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fc0b42d1bd14bc5b7c5a89be6fe6184",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f968a68f41714966a69fac3d235dae23",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "f1f2ac70a2a84b87a13d07842725925e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f652638eb7f24fe4832c8718990848e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f71f3644c615407cabb67993b93d99d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28db79e180c54dccb35605429b5d2ee3",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7dc3d986d00841ada6e9cb437f2dd482",
            "value": 5
          }
        },
        "f968a68f41714966a69fac3d235dae23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa14e496f4b54a84ba3dc8efc509506d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd29a77c3e04332b8f59cefdca1cff6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc19acd2bfdf4f36aff0f7c7c6a5ba7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c588711ce494c17a2e4bcdd270c08fe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9c44b73b4f504b6594444d1fc0609d98",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá65.09‚Äáexamples/s]"
          }
        },
        "fc28946e9ddf4d2a9f56f4b12d3613aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc7ed83cff4f4aaba55f4688e21a27d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fda97e1770e94ac78bd7a3a06e910090": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
